{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtA6DoBc2yaw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n"
      ],
      "metadata": {
        "id": "QQHG7VJ723Qf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression** and **Linear Regression** are both popular statistical models used for prediction, but they are used in different types of problems and differ in how they model relationships between variables.\n",
        "\n",
        "### **Linear Regression**:\n",
        "- **Purpose**: It's used for predicting a continuous (real-valued) outcome based on one or more predictor variables (features).\n",
        "- **Output**: The output is a continuous value, and the model is typically trying to fit a straight line (in the case of one feature) or a hyperplane (in the case of multiple features) to the data.\n",
        "- **Equation**: The equation is of the form:  \n",
        "  \\[\n",
        "  y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon\n",
        "  \\]\n",
        "  Where:\n",
        "  - \\(y\\) is the dependent variable (target).\n",
        "  - \\(\\beta_0, \\beta_1, \\dots\\) are the coefficients.\n",
        "  - \\(x_1, x_2, \\dots\\) are the independent variables (features).\n",
        "  - \\(\\epsilon\\) is the error term.\n",
        "- **Example**: Predicting house prices based on factors like square footage, number of bedrooms, etc.\n",
        "\n",
        "### **Logistic Regression**:\n",
        "- **Purpose**: It's used for classification problems, where the outcome variable is categorical (usually binary, like \"yes\" or \"no,\" or \"0\" or \"1\").\n",
        "- **Output**: The output is a probability value that ranges between 0 and 1. This probability can be thresholded to assign a class label (e.g., if the probability is greater than 0.5, predict \"1\", otherwise predict \"0\").\n",
        "- **Equation**: The logistic regression model applies the logistic function (also known as the sigmoid function) to the linear combination of the features. The equation looks like this:  \n",
        "  \\[\n",
        "  p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)}}\n",
        "  \\]\n",
        "  Where:\n",
        "  - \\(p\\) is the predicted probability that the outcome is \"1\".\n",
        "  - The rest of the terms are the same as in linear regression.\n",
        "- **Example**: Predicting whether an email is spam or not (binary classification), or predicting whether a customer will buy a product (yes/no).\n",
        "\n",
        "### **Key Differences**:\n",
        "1. **Type of Problem**:\n",
        "   - Linear regression: Used for predicting a continuous value.\n",
        "   - Logistic regression: Used for classification (binary or multiclass).\n",
        "   \n",
        "2. **Output**:\n",
        "   - Linear regression: A continuous value (real number).\n",
        "   - Logistic regression: A probability that maps to a category (typically 0 or 1).\n",
        "   \n",
        "3. **Model Equation**:\n",
        "   - Linear regression: A straight line or hyperplane.\n",
        "   - Logistic regression: A sigmoid curve (S-shaped).\n",
        "\n",
        "4. **Error Metric**:\n",
        "   - Linear regression: Typically uses Mean Squared Error (MSE) for evaluation.\n",
        "   - Logistic regression: Typically uses Log Loss (also called Binary Cross-Entropy) or accuracy for evaluation.\n",
        "\n",
        "In summary, the core difference is in the nature of the problem you're trying to solve—linear regression is for regression (continuous) problems, while logistic regression is for classification (categorical) problems."
      ],
      "metadata": {
        "id": "Yn59SsyN25sv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression?\n"
      ],
      "metadata": {
        "id": "oZH4GYay2-vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mathematical equation of **Logistic Regression** is based on the **logistic function** (also known as the **sigmoid function**) that outputs probabilities, mapping the result to a value between 0 and 1.\n",
        "\n",
        "Here's the equation of **Logistic Regression**:\n",
        "\n",
        "### **Logistic Function** (Sigmoid Function):\n",
        "\\[\n",
        "p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n)}}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( p \\) is the predicted probability that the outcome is 1 (i.e., \\( P(y = 1 | x) \\)).\n",
        "- \\( e \\) is Euler's number (approximately 2.718).\n",
        "- \\( \\beta_0 \\) is the intercept (bias term).\n",
        "- \\( \\beta_1, \\beta_2, \\dots, \\beta_n \\) are the coefficients (weights) of the predictor variables \\( x_1, x_2, \\dots, x_n \\), respectively.\n",
        "- \\( x_1, x_2, \\dots, x_n \\) are the input features (predictors).\n",
        "\n",
        "### **Interpretation**:\n",
        "- The expression \\( \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n \\) is a **linear combination** of the input features, just like in linear regression.\n",
        "- The sigmoid function then transforms this linear combination into a probability between 0 and 1. The model predicts \\( p \\), the probability that the output variable \\( y \\) is 1, given the input features \\( x_1, x_2, \\dots, x_n \\).\n",
        "\n",
        "### **Prediction**:\n",
        "- To classify, the output probability \\( p \\) is compared to a threshold (often 0.5). If \\( p \\geq 0.5 \\), the prediction is 1 (positive class), otherwise, the prediction is 0 (negative class).\n",
        "\n",
        "### **Decision Rule**:\n",
        "\\[\n",
        "\\text{If } p \\geq 0.5, \\text{ predict } y = 1\n",
        "\\]\n",
        "\\[\n",
        "\\text{If } p < 0.5, \\text{ predict } y = 0\n",
        "\\]\n",
        "\n",
        "This probabilistic interpretation is why logistic regression is often used in classification tasks, particularly when the goal is to determine the likelihood of an event (e.g., \"yes\" or \"no,\" \"spam\" or \"not spam\")."
      ],
      "metadata": {
        "id": "cHFAvCQ63G-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression?"
      ],
      "metadata": {
        "id": "shfy34QA3O9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the **Sigmoid function** in **Logistic Regression** for several key reasons, mainly due to its properties that make it ideal for classification problems where the output is binary (i.e., 0 or 1).\n",
        "\n",
        "### **Reasons for Using the Sigmoid Function**:\n",
        "\n",
        "1. **Probability Output**:\n",
        "   - The **sigmoid function** maps any real-valued input (from the linear combination of the features) to a value between **0 and 1**. This output can be interpreted as a probability, which is essential for classification tasks where we want to predict the likelihood of a certain class (e.g., the probability that an email is spam or not).\n",
        "   - In essence, it transforms the linear model's output into a probabilistic prediction:  \n",
        "     \\[\n",
        "     p = \\frac{1}{1 + e^{-z}}\n",
        "     \\]\n",
        "     where \\( z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n \\).\n",
        "\n",
        "2. **Squashes the Output**:\n",
        "   - The sigmoid function compresses the entire range of possible outputs (from \\(-\\infty\\) to \\(+\\infty\\)) into a bounded range [0, 1]. This \"squashing\" effect helps in modeling probabilities that can't exceed 1 or go below 0, which is crucial for classification.\n",
        "   \n",
        "3. **Non-linearity**:\n",
        "   - The sigmoid function introduces **non-linearity** to the model. This non-linearity allows logistic regression to model more complex decision boundaries, enabling it to separate classes that may not be linearly separable in the input space.\n",
        "\n",
        "4. **Interpretability**:\n",
        "   - The output of the sigmoid function is often interpreted as the probability that the input belongs to a certain class. For instance, if the output is \\( p = 0.8 \\), we interpret it as an 80% chance that the instance belongs to class 1. This makes the model's predictions intuitive and easy to understand.\n",
        "   \n",
        "5. **Gradient Descent and Optimization**:\n",
        "   - The sigmoid function has a well-behaved **derivative**, which is crucial for **optimization** techniques like **Gradient Descent**. The derivative of the sigmoid function is simple and efficient to compute, allowing the model to learn and update the weights efficiently during training.\n",
        "\n",
        "   The derivative of the sigmoid function is:\n",
        "   \\[\n",
        "   \\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\n",
        "   \\]\n",
        "   where \\( \\sigma(z) \\) is the sigmoid function. This property makes it easy to compute gradients for optimization algorithms.\n",
        "\n",
        "6. **Smooth and Continuous**:\n",
        "   - The sigmoid function is **continuous** and **differentiable** everywhere, which is ideal for optimization. This allows logistic regression to adjust its weights smoothly during training, helping the model converge to an optimal solution without abrupt jumps.\n",
        "\n",
        "7. **Decision Boundary**:\n",
        "   - The sigmoid function defines a clear **decision boundary**. For binary classification, when the probability \\( p \\) reaches 0.5, the model classifies the instance as class 1 (positive class), and below 0.5, it classifies it as class 0 (negative class). This natural thresholding helps in decision-making.\n",
        "\n"
      ],
      "metadata": {
        "id": "seadeHXO3Uqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the cost function of Logistic Regression?\n"
      ],
      "metadata": {
        "id": "he8Lt4oi3ieB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **cost function** (or **loss function**) in **Logistic Regression** is used to measure how well the model's predictions match the actual values. In logistic regression, we are solving a **binary classification problem**, and the cost function helps us minimize the difference between predicted probabilities and the actual labels (0 or 1).\n",
        "\n",
        "### **Logistic Regression Cost Function** (Binary Cross-Entropy):\n",
        "The cost function used in logistic regression is called the **logistic loss** or **binary cross-entropy loss**. It is derived from the likelihood function and measures the difference between the true labels and the predicted probabilities.\n",
        "\n",
        "For a single training example, the cost function is:\n",
        "\n",
        "\\[\n",
        "J(\\theta) = - \\left[ y \\cdot \\log(h_\\theta(x)) + (1 - y) \\cdot \\log(1 - h_\\theta(x)) \\right]\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( y \\) is the actual label (either 0 or 1).\n",
        "- \\( h_\\theta(x) \\) is the predicted probability that \\( y = 1 \\), given the input features \\( x \\), and is calculated using the sigmoid function:\n",
        "  \\[\n",
        "  h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}}\n",
        "  \\]\n",
        "  where \\( \\theta^T x \\) is the linear combination of the features and parameters (weights).\n",
        "  \n"
      ],
      "metadata": {
        "id": "4rARSGoH3oG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed?"
      ],
      "metadata": {
        "id": "cFYnd6qg3_qT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization** in **Logistic Regression** is a technique used to prevent **overfitting** by adding a penalty term to the cost function. The idea is to discourage the model from becoming too complex, which can lead to poor generalization to new, unseen data. Regularization achieves this by penalizing large model coefficients (parameters), encouraging simpler models that generalize better.\n",
        "\n",
        "### **Why is Regularization Needed in Logistic Regression?**\n",
        "\n",
        "1. **Overfitting Prevention**:\n",
        "   - **Overfitting** occurs when the model fits the training data too closely, capturing noise and random fluctuations in the data rather than the underlying trend. This results in poor performance on unseen data because the model is too specialized to the training set.\n",
        "   - Regularization helps by reducing the complexity of the model, making it less likely to overfit, and improving the model's ability to generalize.\n",
        "\n",
        "2. **Control Model Complexity**:\n",
        "   - A model with many features (predictors) may learn overly complex relationships, making the model more prone to overfitting.\n",
        "   - Regularization encourages smaller model parameters, which simplifies the model and forces it to focus on the most important features, preventing it from relying too heavily on individual features that may just be noise.\n",
        "\n",
        "3. **Prevents Large Weights**:\n",
        "   - Without regularization, the parameters of the model (\\( \\theta \\)) can grow very large if the training data is noisy or the model is overly complex.\n",
        "   - Large weights can lead to unstable predictions, especially for new data.\n",
        "   - Regularization keeps the weights smaller, making the model more stable and robust.\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "85x3Gw3g4E-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression?\n"
      ],
      "metadata": {
        "id": "QIbHEqFS4WMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lasso, Ridge, and Elastic Net regression** are all variations of **regularized linear regression** methods that add penalty terms to the cost function in order to prevent overfitting and improve generalization. They differ in the way the regularization term is applied to the coefficients (parameters) of the model.\n",
        "\n",
        "### **1. Ridge Regression (L2 Regularization)**\n",
        "\n",
        "- **Penalty term**: Adds the **sum of the squared values** of the model coefficients to the cost function. This is known as **L2 regularization**.\n",
        "  \n",
        "- **Cost function**:\n",
        "  \\[\n",
        "  J(\\theta) = \\text{RSS} + \\lambda \\sum_{j=1}^{n} \\theta_j^2\n",
        "  \\]\n",
        "  Where:\n",
        "  - \\( \\text{RSS} \\) is the residual sum of squares (the error term of the model).\n",
        "  - \\( \\theta_j \\) are the model parameters.\n",
        "  - \\( \\lambda \\) is the regularization parameter controlling the strength of the penalty.\n",
        "\n",
        "- **Effect on coefficients**:\n",
        "  - **Ridge regression** penalizes large coefficients by shrinking them, but it **never forces them to zero**. It keeps all features in the model, just making their coefficients smaller. This is useful when you believe that most features are relevant and you want to reduce the impact of less important features without discarding them.\n",
        "\n",
        "- **When to use**:\n",
        "  - Use **Ridge regression** when you believe there is **multicollinearity** (high correlation between features) in your data and want to prevent large coefficients but don't want to eliminate any features entirely.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Lasso Regression (L1 Regularization)**\n",
        "\n",
        "- **Penalty term**: Adds the **sum of the absolute values** of the model coefficients to the cost function. This is known as **L1 regularization**.\n",
        "\n",
        "- **Cost function**:\n",
        "  \\[\n",
        "  J(\\theta) = \\text{RSS} + \\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
        "  \\]\n",
        "  Where:\n",
        "  - \\( \\text{RSS} \\) is the residual sum of squares (the error term of the model).\n",
        "  - \\( \\theta_j \\) are the model parameters.\n",
        "  - \\( \\lambda \\) is the regularization parameter controlling the strength of the penalty.\n",
        "\n",
        "- **Effect on coefficients**:\n",
        "  - **Lasso regression** tends to **drive some coefficients exactly to zero**, effectively **performing feature selection**. It will completely eliminate less important features by setting their coefficients to zero.\n",
        "  - This can be very helpful when you believe that some features are irrelevant or redundant, and you want to automatically exclude them from the model.\n",
        "\n",
        "- **When to use**:\n",
        "  - Use **Lasso regression** when you suspect that many features are irrelevant or when you need **automatic feature selection**.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Elastic Net Regression**\n",
        "\n",
        "- **Penalty term**: Elastic Net combines both **L1 (Lasso)** and **L2 (Ridge)** penalties into a single regularization term. It uses a linear combination of both Lasso and Ridge regularization.\n",
        "\n",
        "- **Cost function**:\n",
        "  \\[\n",
        "  J(\\theta) = \\text{RSS} + \\lambda_1 \\sum_{j=1}^{n} |\\theta_j| + \\frac{\\lambda_2}{2} \\sum_{j=1}^{n} \\theta_j^2\n",
        "  \\]\n",
        "  Where:\n",
        "  - \\( \\text{RSS} \\) is the residual sum of squares (the error term of the model).\n",
        "  - \\( \\theta_j \\) are the model parameters.\n",
        "  - \\( \\lambda_1 \\) and \\( \\lambda_2 \\) are the regularization parameters that control the strength of the **L1** and **L2** penalties, respectively.\n",
        "\n",
        "- **Effect on coefficients**:\n",
        "  - Elastic Net encourages both **shrinkage** (like Ridge) and **sparsity** (like Lasso). It allows some coefficients to be shrunk towards zero, but some can also be completely driven to zero.\n",
        "  - This combination is particularly useful when you have a large number of correlated features or when you want a balance between feature selection and shrinkage.\n",
        "\n",
        "- **When to use**:\n",
        "  - Use **Elastic Net** when you have a mix of **relevant and irrelevant features** or when you have **highly correlated features**. It is often preferred when you want to combine the advantages of both **Lasso** (feature selection) and **Ridge** (shrinkage of coefficients).\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Differences**:\n",
        "\n",
        "| **Property**         | **Ridge Regression (L2)**                      | **Lasso Regression (L1)**                    | **Elastic Net**                               |\n",
        "|----------------------|------------------------------------------------|---------------------------------------------|------------------------------------------------|\n",
        "| **Penalty term**      | \\( \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\)       | \\( \\lambda \\sum_{j=1}^{n} |\\theta_j| \\)     | \\( \\lambda_1 \\sum_{j=1}^{n} |\\theta_j| + \\frac{\\lambda_2}{2} \\sum_{j=1}^{n} \\theta_j^2 \\) |\n",
        "| **Impact on coefficients** | Shrinks all coefficients but does not eliminate them | Shrinks some coefficients to zero, performs feature selection | Combines both L1 and L2, can shrink some coefficients and eliminate others |\n",
        "| **When to use**       | When you want to reduce multicollinearity but retain all features | When you want to select important features and eliminate irrelevant ones | When you want a balance between Ridge and Lasso, especially with correlated features |\n",
        "| **Feature selection** | No automatic feature selection | Automatically performs feature selection by setting some coefficients to zero | Performs feature selection and shrinkage together |\n",
        "| **Use cases**         | High-dimensional data with correlated predictors | Sparse models with fewer features | Data with many correlated features, when neither Lasso nor Ridge is perfect |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j4nsVrZk4dVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 7. When should we use Elastic Net instead of Lasso or Ridge?"
      ],
      "metadata": {
        "id": "cDYSxfKU4pyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should use **Elastic Net** instead of **Lasso** or **Ridge** regression in specific scenarios where both the advantages of **L1** (Lasso) and **L2** (Ridge) regularization are needed. Elastic Net combines both regularization techniques, making it a versatile choice for certain types of data and problems.\n",
        "\n",
        "Here’s when to use **Elastic Net** over **Lasso** or **Ridge**:\n",
        "\n",
        "### 1. **When You Have Highly Correlated Features**:\n",
        "- **Elastic Net** is particularly useful when the dataset contains **highly correlated features**.\n",
        "- **Lasso** tends to select only one feature from a group of highly correlated features and set the others to zero, while **Ridge** tends to shrink all of them evenly but doesn't set any coefficients to exactly zero.\n",
        "- **Elastic Net** performs **both shrinkage and feature selection**, meaning it can both reduce the magnitude of correlated features' coefficients (like Ridge) and potentially eliminate some of them (like Lasso).\n",
        "- This is particularly helpful when you don’t know which features are highly correlated, or when there are many correlated predictors in the data.\n",
        "\n",
        "### 2. **When You Have a Large Number of Features (High-Dimensional Data)**:\n",
        "- In high-dimensional settings (where the number of features \\(p\\) is greater than the number of samples \\(n\\)), **Elastic Net** performs better than **Lasso** because it can handle correlated features while still performing feature selection.\n",
        "- **Lasso** may not work well in such situations because it can arbitrarily choose one feature from a group of correlated predictors, which may result in instability or inconsistent feature selection across different datasets.\n",
        "- **Elastic Net** mitigates this issue by combining **L1** and **L2** regularization, which encourages **grouped selection** of correlated features while still promoting sparsity.\n",
        "\n",
        "### 3. **When You Want the Benefits of Both Lasso and Ridge**:\n",
        "- **Lasso** and **Ridge** have different strengths: Lasso is good for performing **automatic feature selection** (by setting some coefficients to zero), while Ridge is good at **shrinking coefficients** to prevent overfitting without removing features entirely.\n",
        "- **Elastic Net** provides a combination of both by using a mix of **L1** and **L2** penalties. It allows you to perform feature selection **(like Lasso)** and shrinkage **(like Ridge)** simultaneously, making it an ideal choice when you need both.\n",
        "  \n",
        "### 4. **When You Suspect Many Irrelevant Features**:\n",
        "- If you suspect that many of your features are irrelevant (which is often the case in high-dimensional datasets), **Elastic Net** can help by **shrinking the coefficients** of less important features (like Ridge) and **driving some of them to zero** (like Lasso).\n",
        "- This combination helps to keep your model simpler, making it more interpretable and preventing overfitting, especially in cases with many features relative to the number of samples.\n",
        "\n",
        "### 5. **When You Want to Avoid Overfitting with Regularization**:\n",
        "- When your model is at risk of **overfitting** due to a large number of features, **Elastic Net** can help control the complexity of the model by combining the strengths of **L2 shrinkage** (Ridge) and **L1 sparsity** (Lasso).\n",
        "- This is particularly important if you have a large dataset with many features, where both overfitting and underfitting are risks.\n",
        "\n",
        "### 6. **When You Don’t Know Which Regularization Technique Will Work Best**:\n",
        "- If you're unsure whether **Lasso** or **Ridge** is more appropriate for your data, **Elastic Net** gives you the flexibility to benefit from both. It allows you to tune the relative importance of **L1** and **L2** regularization via the hyperparameters \\( \\lambda_1 \\) and \\( \\lambda_2 \\).\n",
        "- This makes **Elastic Net** a good choice when you are uncertain which technique will yield better results and want to combine both approaches.\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Prefer Lasso or Ridge Instead**:\n",
        "\n",
        "- **Use Lasso**:\n",
        "  - When you have **sparse data** or you believe that **many features are irrelevant** and should be excluded (i.e., you want **automatic feature selection**).\n",
        "  - When you have **fewer features** and **no multicollinearity** or only slight correlation between predictors.\n",
        "  - If you prefer a **simpler model** with only a subset of the most important features.\n",
        "\n",
        "- **Use Ridge**:\n",
        "  - When you have **highly correlated features** but still believe all features should be included in the model.\n",
        "  - When you don’t expect many features to be irrelevant and prefer to shrink coefficients rather than eliminate them.\n",
        "  - If you're dealing with **multicollinearity** (i.e., high correlation among features) and want to avoid overfitting without reducing the number of features.\n",
        "\n"
      ],
      "metadata": {
        "id": "MaGkQvOj4xGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression?"
      ],
      "metadata": {
        "id": "NXxK_GmA47Ot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The regularization parameter, denoted as \\( \\lambda \\), plays a crucial role in **controlling the strength** of regularization in **Logistic Regression**. Regularization is used to prevent **overfitting** and improve the generalization of the model by penalizing large coefficients. Here's an explanation of how \\( \\lambda \\) affects the model:\n",
        "\n",
        "### **1. Control of Regularization Strength**\n",
        "\n",
        "- The **regularization parameter \\( \\lambda \\)** controls how strongly regularization affects the model. It determines the **penalty** added to the cost function. The larger the value of \\( \\lambda \\), the stronger the penalty on the model's coefficients.\n",
        "\n",
        "### **2. When \\( \\lambda = 0 \\)**:\n",
        "- **No Regularization**:\n",
        "  - If \\( \\lambda = 0 \\), there is **no regularization** applied to the model, and the logistic regression behaves like an **unregularized logistic regression model**.\n",
        "  - The model will try to **fit the training data as closely as possible**, which can lead to **overfitting**, especially if there are many features or the dataset is noisy.\n",
        "\n",
        "  **Impact**:\n",
        "  - Coefficients can become large, especially in high-dimensional data, making the model **sensitive to small fluctuations in the data**, leading to poor generalization to new, unseen data.\n",
        "\n",
        "### **3. When \\( \\lambda \\) is Large**:\n",
        "- **Strong Regularization**:\n",
        "  - When \\( \\lambda \\) is large, the regularization term in the cost function becomes dominant, and the model will try to **shrink the coefficients towards zero**.\n",
        "  - **L1 regularization (Lasso)** will drive some coefficients **exactly to zero**, eliminating those features from the model. This leads to **sparse models** with fewer features.\n",
        "  - **L2 regularization (Ridge)** will shrink the coefficients without driving them to zero, leading to a **simpler model** with smaller weights.\n",
        "  - **Elastic Net**, which combines both L1 and L2 regularization, will result in a **mix of coefficient shrinkage** and **feature selection**.\n",
        "\n",
        "  **Impact**:\n",
        "  - The model becomes **simpler** and more **robust** to overfitting, as it avoids relying too heavily on any particular feature.\n",
        "  - However, if \\( \\lambda \\) is too large, the model may become too simple and may **underfit** the data. This means the model might not capture the underlying patterns in the data and may have **low training accuracy**.\n",
        "\n",
        "### **4. When \\( \\lambda \\) is Small**:\n",
        "- **Weak Regularization**:\n",
        "  - When \\( \\lambda \\) is small (but greater than 0), regularization has a **mild effect** on the model. The coefficients are only slightly penalized, allowing the model to focus on minimizing the error while still keeping the coefficients relatively large.\n",
        "\n",
        "  **Impact**:\n",
        "  - The model has the flexibility to learn more complex relationships in the data and may fit the training data more closely.\n",
        "  - However, if \\( \\lambda \\) is too small, the model is more likely to **overfit** the data, especially if the data is noisy or there are many irrelevant features.\n",
        "\n",
        "### **Summary of Effects of \\( \\lambda \\)**:\n",
        "\n",
        "| **\\( \\lambda \\) Value**     | **Effect on Model**                                                                 | **Outcome**                                      |\n",
        "|-----------------------------|--------------------------------------------------------------------------------------|--------------------------------------------------|\n",
        "| **\\( \\lambda = 0 \\)**        | No regularization (standard logistic regression).                                   | Risk of **overfitting** due to large coefficients. |\n",
        "| **Small \\( \\lambda \\)**      | Mild regularization, coefficients are slightly penalized.                           | **Underfitting** risk reduced, but overfitting can occur. |\n",
        "| **Large \\( \\lambda \\)**      | Strong regularization, coefficients are heavily penalized, potentially zeroed out. | **Underfitting** risk increased, model is too simple. |\n",
        "| **Optimal \\( \\lambda \\)**    | Right balance between model flexibility and regularization.                        | **Best generalization**, reducing both overfitting and underfitting. |\n",
        "\n",
        "### **5. Finding the Optimal \\( \\lambda \\) (Tuning Hyperparameter)**:\n",
        "\n",
        "The value of \\( \\lambda \\) is typically selected using techniques such as **cross-validation** or **grid search**. By testing various values of \\( \\lambda \\), you can find the one that provides the **best balance** between model complexity (fitting the data well) and regularization (preventing overfitting).\n",
        "\n",
        "### **6. Impact of Regularization in Different Contexts**:\n",
        "\n",
        "- **With L1 Regularization (Lasso)**: When \\( \\lambda \\) is large, many coefficients will be set to zero, effectively removing features from the model. If \\( \\lambda \\) is small, Lasso behaves like regular logistic regression, using all features.\n",
        "  \n",
        "- **With L2 Regularization (Ridge)**: With a larger \\( \\lambda \\), Ridge regression shrinks all coefficients toward zero, making them smaller and less sensitive to the training data. If \\( \\lambda \\) is small, the model behaves similarly to regular logistic regression, fitting the data more closely.\n",
        "\n",
        "- **With Elastic Net**: \\( \\lambda_1 \\) (for Lasso) and \\( \\lambda_2 \\) (for Ridge) control the combination of the two penalties. By tuning these parameters, you can get the right balance between feature selection and shrinkage.\n",
        "\n",
        "### **In Summary**:\n",
        "\n",
        "- The regularization parameter \\( \\lambda \\) directly influences the model's **complexity** and its **ability to generalize**.\n",
        "- A **small \\( \\lambda \\)** can lead to **overfitting** as the model fits the data too closely, while a **large \\( \\lambda \\)** can lead to **underfitting** by simplifying the model too much.\n",
        "- The optimal value of \\( \\lambda \\) is typically found through experimentation and model validation, aiming to achieve the best trade-off between fitting the training data and generalizing well to new data."
      ],
      "metadata": {
        "id": "7RfscxcB4_gD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression?"
      ],
      "metadata": {
        "id": "MnqyizCs5L08"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression** is a widely used statistical model for binary classification problems. Despite its simplicity and popularity, it relies on several key assumptions to provide reliable results. These assumptions are important to ensure the model's validity and performance. Here are the key assumptions of **Logistic Regression**:\n",
        "\n",
        "### **1. Linearity of the Log-Odds (Logit)**\n",
        "- **Assumption**: The relationship between the independent variables (predictors) and the **log-odds** (logarithm of the odds) of the dependent variable is linear.\n",
        "- **Explanation**: In logistic regression, the log of the odds of the outcome is modeled as a linear combination of the independent variables. Specifically, the equation is:\n",
        "  \\[\n",
        "  \\text{logit}(P(y=1)) = \\ln\\left(\\frac{P(y=1)}{1 - P(y=1)}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
        "  \\]\n",
        "  This means that the model assumes that the log-odds of the dependent variable are a linear function of the independent variables.\n",
        "\n",
        "  **Why it matters**: If this assumption is violated (i.e., if the relationship is not linear), the model may not capture the underlying data patterns, leading to **biased estimates**.\n",
        "\n",
        "### **2. Independence of Errors (No autocorrelation)**\n",
        "- **Assumption**: The residuals (errors) of the model should be **independent** of each other.\n",
        "- **Explanation**: This means that the errors should not exhibit patterns or correlation between the observations. In other words, the error for one observation should not depend on the error for another observation.\n",
        "\n",
        "  **Why it matters**: Violation of this assumption (such as in time-series data where errors may be correlated over time) can lead to **incorrect standard errors**, **inflated significance levels**, and **biased coefficient estimates**.\n",
        "\n",
        "### **3. No Multicollinearity**\n",
        "- **Assumption**: The independent variables should not be **highly correlated** with each other.\n",
        "- **Explanation**: Logistic regression assumes that the predictors (independent variables) are **linearly independent**. High correlations between predictors (multicollinearity) can make it difficult to estimate the individual effect of each variable.\n",
        "\n",
        "  **Why it matters**: Multicollinearity leads to **unstable estimates** of the coefficients and **increased variance**, making the model difficult to interpret. This can cause the model to assign incorrect importance to the correlated variables.\n",
        "\n",
        "### **4. Large Sample Size**\n",
        "- **Assumption**: Logistic regression requires a sufficiently large sample size for reliable parameter estimation.\n",
        "- **Explanation**: Logistic regression relies on **maximum likelihood estimation** (MLE) to estimate the model parameters. MLE is an iterative process, and it works best with a large sample size, particularly when the number of predictors is high.\n",
        "\n",
        "  **Why it matters**: With small sample sizes, the model's estimates may become **unstable** or **biased**. Additionally, with too few observations, the **variance** of the estimates may be large, making the model less reliable.\n",
        "\n",
        "### **5. The Dependent Variable is Binary**\n",
        "- **Assumption**: Logistic regression assumes that the dependent variable is **binary** (i.e., takes two possible outcomes, such as 0 or 1, success or failure, etc.).\n",
        "- **Explanation**: The goal of logistic regression is to model the **probability** of a binary outcome. If the dependent variable has more than two categories (multinomial), logistic regression would need to be extended (e.g., **Multinomial Logistic Regression**).\n",
        "\n",
        "  **Why it matters**: Logistic regression is specifically designed for binary classification. For more than two categories, other models like **Multinomial Logistic Regression** or **Ordinal Regression** may be more appropriate.\n",
        "\n",
        "### **6. No Outliers in the Data**\n",
        "- **Assumption**: Logistic regression assumes that there are **no extreme outliers** in the data.\n",
        "- **Explanation**: While logistic regression is generally more robust to outliers than linear regression, **outliers** in the predictors can still **distort** the estimated coefficients, especially when they have a disproportionate effect on the log-odds.\n",
        "\n",
        "  **Why it matters**: Outliers can lead to **incorrect conclusions** about the relationship between predictors and the outcome. It's good practice to inspect the data for outliers before fitting a logistic regression model.\n",
        "\n",
        "### **7. Homogeneity of Variance (Homoscedasticity) of Errors**\n",
        "- **Assumption**: While **homoscedasticity** is a common assumption in **linear regression**, it is **not strictly required** for logistic regression because the outcome is binary. The variance of the error terms does not need to be constant.\n",
        "- **Explanation**: Logistic regression works with the probabilities of binary outcomes, which are assumed to have a variance that depends on the predicted probability (i.e., the variance of errors is non-constant and dependent on the fitted values).\n",
        "\n",
        "  **Why it matters**: Since logistic regression is based on likelihood estimation, the assumption of homoscedasticity is not as critical as it is in linear regression. However, violations can affect **model precision** and **standard error estimates**.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NMF4I1L55QWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are some alternatives to Logistic Regression for classification tasks?"
      ],
      "metadata": {
        "id": "eRH1-i8I6OaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When **Logistic Regression** is not suitable or when you want to explore different approaches for classification tasks, several **alternatives** can be considered. These alternatives may offer advantages in terms of handling non-linear relationships, dealing with complex data, or improving model performance. Here's a list of common alternatives:\n",
        "\n",
        "### **1. Decision Trees**\n",
        "- **Overview**: Decision Trees are a non-linear classifier that splits the data into subsets based on feature values, building a tree-like structure of decisions.\n",
        "- **How it works**: At each internal node, the tree selects the feature and threshold that best splits the data into classes (based on measures like **Gini impurity** or **entropy**).\n",
        "- **Advantages**:\n",
        "  - Easy to interpret.\n",
        "  - Handles both numerical and categorical data.\n",
        "  - Can capture non-linear relationships.\n",
        "- **Disadvantages**:\n",
        "  - Prone to overfitting, especially with deep trees.\n",
        "  - Less stable (small changes in data can lead to a different tree structure).\n",
        "\n",
        "### **2. Random Forest**\n",
        "- **Overview**: Random Forest is an ensemble method based on **decision trees**, where multiple trees are trained on random subsets of the data, and their predictions are averaged (for regression) or voted upon (for classification).\n",
        "- **How it works**: Each tree in the forest is built on a random subset of features, and the final prediction is determined by a majority vote.\n",
        "- **Advantages**:\n",
        "  - Reduces overfitting compared to a single decision tree.\n",
        "  - Handles high-dimensional data well.\n",
        "  - Provides feature importance.\n",
        "- **Disadvantages**:\n",
        "  - Less interpretable than a single decision tree.\n",
        "  - Computationally expensive, especially with large datasets.\n",
        "\n",
        "### **3. Support Vector Machines (SVM)**\n",
        "- **Overview**: SVM is a powerful classifier that works by finding the optimal hyperplane that separates classes in the feature space.\n",
        "- **How it works**: SVM tries to find the **hyperplane** that maximizes the margin between the classes. For non-linearly separable data, it uses kernel tricks (e.g., **RBF kernel**) to transform the data into a higher-dimensional space.\n",
        "- **Advantages**:\n",
        "  - Effective for high-dimensional spaces.\n",
        "  - Can model complex decision boundaries with kernels.\n",
        "  - Works well with smaller datasets and is less prone to overfitting.\n",
        "- **Disadvantages**:\n",
        "  - Computationally expensive with large datasets.\n",
        "  - Sensitive to the choice of kernel and hyperparameters.\n",
        "  - Difficult to interpret, especially in non-linear settings.\n",
        "\n",
        "### **4. k-Nearest Neighbors (k-NN)**\n",
        "- **Overview**: k-NN is a **lazy learning algorithm** that assigns a class based on the majority class of the \\( k \\) nearest neighbors in the feature space.\n",
        "- **How it works**: For each new data point, k-NN computes the distances to all training points, selects the \\( k \\) closest ones, and assigns the class based on the most frequent class among those neighbors.\n",
        "- **Advantages**:\n",
        "  - Simple to understand and implement.\n",
        "  - Non-parametric, so no assumptions about data distribution.\n",
        "  - Works well for smaller datasets.\n",
        "- **Disadvantages**:\n",
        "  - Computationally expensive during prediction, especially with large datasets.\n",
        "  - Sensitive to the choice of \\( k \\) and the distance metric.\n",
        "  - Struggles with high-dimensional data due to the **curse of dimensionality**.\n",
        "\n",
        "### **5. Naive Bayes**\n",
        "- **Overview**: Naive Bayes is a probabilistic classifier based on Bayes’ Theorem, assuming **conditional independence** between features given the class label.\n",
        "- **How it works**: The model calculates the posterior probability of each class using the likelihood of each feature and applies **Bayes' Theorem**.\n",
        "- **Advantages**:\n",
        "  - Simple and fast, especially for large datasets.\n",
        "  - Works well when features are conditionally independent.\n",
        "  - Effective in text classification (e.g., spam detection) and other categorical data problems.\n",
        "- **Disadvantages**:\n",
        "  - Assumption of feature independence is often unrealistic.\n",
        "  - Struggles with highly correlated features.\n",
        "  - Less flexible in modeling complex relationships between features.\n",
        "\n",
        "### **6. Gradient Boosting Machines (GBM)**\n",
        "- **Overview**: Gradient Boosting is an ensemble method that builds trees in a **sequential manner**, where each tree tries to correct the errors made by the previous tree.\n",
        "- **How it works**: Each successive tree focuses on the **residual errors** (misclassified points) of the previous trees, and the predictions are combined in an additive manner.\n",
        "- **Advantages**:\n",
        "  - Often yields state-of-the-art performance in many tasks.\n",
        "  - Effective with large and complex datasets.\n",
        "  - Can model non-linear relationships.\n",
        "- **Disadvantages**:\n",
        "  - Prone to overfitting if not tuned carefully.\n",
        "  - Computationally expensive, especially for large datasets.\n",
        "  - Requires careful hyperparameter tuning.\n",
        "\n",
        "### **7. XGBoost (Extreme Gradient Boosting)**\n",
        "- **Overview**: XGBoost is an optimized implementation of Gradient Boosting that is widely used for structured/tabular data classification.\n",
        "- **How it works**: It uses a gradient boosting framework along with techniques like **regularization**, **tree pruning**, and **early stopping** to prevent overfitting and improve speed.\n",
        "- **Advantages**:\n",
        "  - High performance and speed.\n",
        "  - Handles missing values, regularization, and feature importance.\n",
        "  - Often outperforms other models on structured/tabular data.\n",
        "- **Disadvantages**:\n",
        "  - Requires careful tuning of hyperparameters.\n",
        "  - Less interpretable than simpler models like decision trees.\n",
        "\n",
        "### **8. Neural Networks (Deep Learning)**\n",
        "- **Overview**: Neural Networks are a family of models inspired by the human brain and are used for modeling highly complex patterns in data.\n",
        "- **How it works**: A neural network consists of layers of neurons, where each neuron applies a transformation to the input data and passes it to the next layer. The network is trained via backpropagation.\n",
        "- **Advantages**:\n",
        "  - Very flexible and can model complex non-linear relationships.\n",
        "  - Can learn from large, high-dimensional datasets (e.g., image, speech, or text).\n",
        "  - State-of-the-art performance on tasks like image classification, speech recognition, and natural language processing.\n",
        "- **Disadvantages**:\n",
        "  - Requires large amounts of data and computational resources.\n",
        "  - Difficult to interpret (often referred to as a “black box” model).\n",
        "  - Training can be slow and hyperparameter tuning can be challenging.\n",
        "\n",
        "### **9. AdaBoost (Adaptive Boosting)**\n",
        "- **Overview**: AdaBoost is an ensemble method that combines weak learners (often decision trees) to form a strong classifier. It works by focusing more on misclassified examples in each iteration.\n",
        "- **How it works**: The algorithm trains a sequence of weak models, each correcting the errors of the previous one, and combines them with weighted voting.\n",
        "- **Advantages**:\n",
        "  - Can significantly improve the performance of weak models.\n",
        "  - Simple to implement.\n",
        "  - Reduces bias and variance.\n",
        "- **Disadvantages**:\n",
        "  - Prone to overfitting if too many iterations are used.\n",
        "  - Sensitive to noisy data and outliers.\n",
        "\n",
        "### **10. Linear Discriminant Analysis (LDA)**\n",
        "- **Overview**: LDA is a classification method that seeks to find a linear combination of features that best separates two or more classes.\n",
        "- **How it works**: LDA assumes that each class follows a **Gaussian distribution** and tries to maximize the separation between the classes by minimizing within-class variance.\n",
        "- **Advantages**:\n",
        "  - Simple and computationally efficient.\n",
        "  - Can work well if the classes are normally distributed and have similar covariances.\n",
        "- **Disadvantages**:\n",
        "  - Assumes normality and equal variances, which may not hold in all cases.\n",
        "  - Less effective with complex relationships between features.\n",
        "\n",
        "---\n",
        "\n",
        "### **Choosing the Right Model**:\n",
        "- **For linear relationships** and interpretability, **Logistic Regression**, **LDA**, or **Naive Bayes** are good choices.\n",
        "- **For non-linear relationships** and higher accuracy, **Random Forest**, **SVM**, **XGBoost**, or **Gradient Boosting** may perform better.\n",
        "- If you have **high-dimensional data** or need state-of-the-art performance, **Neural Networks** and **XGBoost** might be the best choice.\n",
        "- **For simplicity and speed**, **k-NN** and **Decision Trees** can be used, though they may require more careful tuning to avoid overfitting.\n",
        "\n",
        "Ultimately, the choice of model depends on factors like **data size**, **complexity**, **interpretability**, and **computational resources**."
      ],
      "metadata": {
        "id": "LOeS6-KD6fdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics?"
      ],
      "metadata": {
        "id": "XttLfaab6wsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification Evaluation Metrics** are crucial for assessing how well a classification model performs on a given dataset. These metrics help to understand the accuracy, precision, recall, and other aspects of the model's performance, especially in terms of its ability to correctly classify instances of each class. Here are the main classification evaluation metrics:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Accuracy**\n",
        "- **Definition**: The proportion of correctly classified instances out of all instances in the dataset.\n",
        "- **Formula**:\n",
        "  \\[\n",
        "  \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n",
        "  \\]\n",
        "  where:\n",
        "  - **TP** = True Positives (correctly predicted positive instances)\n",
        "  - **TN** = True Negatives (correctly predicted negative instances)\n",
        "  - **FP** = False Positives (incorrectly predicted as positive)\n",
        "  - **FN** = False Negatives (incorrectly predicted as negative)\n",
        "\n",
        "- **Usefulness**:\n",
        "  - Simple and widely used.\n",
        "  - Best used when the classes are balanced (i.e., both positive and negative classes have roughly the same number of instances).\n",
        "  \n",
        "- **Limitation**:\n",
        "  - Accuracy can be misleading in imbalanced datasets (where one class dominates the other), as it may appear high even if the model is not performing well on the minority class.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Precision**\n",
        "- **Definition**: The proportion of correctly predicted positive instances out of all instances that were predicted as positive.\n",
        "- **Formula**:\n",
        "  \\[\n",
        "  \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
        "  \\]\n",
        "  \n",
        "- **Usefulness**:\n",
        "  - Important when the cost of **false positives** (e.g., classifying a negative instance as positive) is high.\n",
        "  - Precision is used in contexts where you want to ensure that when the model predicts a positive class, it's truly positive (e.g., in spam detection).\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Recall (Sensitivity or True Positive Rate)**\n",
        "- **Definition**: The proportion of correctly predicted positive instances out of all actual positive instances.\n",
        "- **Formula**:\n",
        "  \\[\n",
        "  \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
        "  \\]\n",
        "  \n",
        "- **Usefulness**:\n",
        "  - Recall is important when the cost of **false negatives** (e.g., classifying a positive instance as negative) is high.\n",
        "  - Used in medical testing or fraud detection, where you don't want to miss any true positives (e.g., detecting a disease or fraud).\n",
        "\n",
        "---\n",
        "\n",
        "### **4. F1 Score**\n",
        "- **Definition**: The harmonic mean of **Precision** and **Recall**, balancing the two metrics. It is useful when you need a balance between Precision and Recall.\n",
        "- **Formula**:\n",
        "  \\[\n",
        "  \\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "  \\]\n",
        "  \n",
        "- **Usefulness**:\n",
        "  - The **F1 score** is a single metric that captures both Precision and Recall, making it useful when the classes are imbalanced and you care about both false positives and false negatives.\n",
        "  - Commonly used when there is a need to balance the trade-off between Precision and Recall.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Specificity (True Negative Rate)**\n",
        "- **Definition**: The proportion of correctly predicted negative instances out of all actual negative instances.\n",
        "- **Formula**:\n",
        "  \\[\n",
        "  \\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\n",
        "  \\]\n",
        "\n",
        "- **Usefulness**:\n",
        "  - Specificity is useful in scenarios where you want to focus on correctly identifying negative instances, such as in fraud detection where you don't want to mistakenly flag a legitimate transaction as fraudulent.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. ROC Curve (Receiver Operating Characteristic Curve)**\n",
        "- **Definition**: A graphical plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied.\n",
        "- **Key Elements**:\n",
        "  - **True Positive Rate (TPR)** or **Recall** on the Y-axis.\n",
        "  - **False Positive Rate (FPR)** on the X-axis.\n",
        "  - The **curve** shows the trade-off between **Recall** and **False Positive Rate** at different threshold settings.\n",
        "\n",
        "- **Usefulness**:\n",
        "  - The ROC curve provides a comprehensive view of the classifier’s performance across different thresholds, helping to evaluate how well the model discriminates between classes.\n",
        "  - The area under the curve (AUC) is a common metric to summarize the ROC curve performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. AUC-ROC (Area Under the ROC Curve)**\n",
        "- **Definition**: The area under the ROC curve, also known as **AUC**, represents the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance.\n",
        "- **Formula**:\n",
        "  - AUC is the area under the ROC curve, which ranges from 0 to 1.\n",
        "  \n",
        "- **Usefulness**:\n",
        "  - AUC is a single value that summarizes the performance of the model. A higher AUC value indicates a better model.\n",
        "  - AUC is especially useful when comparing models, as it is independent of the classification threshold and works well in imbalanced datasets.\n",
        "\n",
        "- **Interpretation**:\n",
        "  - **AUC = 0.5**: The model performs no better than random chance.\n",
        "  - **AUC = 1**: The model perfectly classifies the data.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Confusion Matrix**\n",
        "- **Definition**: A table that summarizes the performance of a classification algorithm by comparing predicted and actual labels. It shows the number of:\n",
        "  - **True Positives (TP)**\n",
        "  - **True Negatives (TN)**\n",
        "  - **False Positives (FP)**\n",
        "  - **False Negatives (FN)**\n",
        "\n",
        "- **Usefulness**:\n",
        "  - It provides a comprehensive view of how the model performs on each class, making it easier to calculate other evaluation metrics (like Precision, Recall, F1 Score).\n",
        "  - It helps identify whether the model is misclassifying one class as another.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Matthews Correlation Coefficient (MCC)**\n",
        "- **Definition**: A balanced measure that takes into account all four categories of the confusion matrix. It’s especially useful in imbalanced datasets.\n",
        "- **Formula**:\n",
        "  \\[\n",
        "  \\text{MCC} = \\frac{\\text{TP} \\cdot \\text{TN} - \\text{FP} \\cdot \\text{FN}}{\\sqrt{(\\text{TP} + \\text{FP})(\\text{TP} + \\text{FN})(\\text{TN} + \\text{FP})(\\text{TN} + \\text{FN})}}\n",
        "  \\]\n",
        "  \n",
        "- **Usefulness**:\n",
        "  - MCC produces a value between -1 and 1:\n",
        "    - **1** indicates a perfect prediction.\n",
        "    - **0** indicates random prediction.\n",
        "    - **-1** indicates complete disagreement between predicted and actual labels.\n",
        "  - MCC is particularly useful for evaluating performance on **imbalanced datasets**.\n",
        "\n",
        "---\n",
        "\n",
        "### **10. Log Loss (Logarithmic Loss or Cross-Entropy Loss)**\n",
        "- **Definition**: Measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
        "- **Formula**:\n",
        "  \\[\n",
        "  \\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]\n",
        "  \\]\n",
        "  where \\( y_i \\) is the true label and \\( \\hat{y}_i \\) is the predicted probability for class 1.\n",
        "\n",
        "- **Usefulness**:\n",
        "  - Log Loss is particularly useful when the classifier provides **probabilities** rather than discrete predictions. It penalizes wrong predictions more heavily when the predicted probability is confident but wrong.\n",
        "  - A **lower Log Loss** indicates better performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **11. Precision-Recall Curve**\n",
        "- **Definition**: A plot that illustrates the trade-off between **Precision** and **Recall** for different threshold values.\n",
        "- **Usefulness**:\n",
        "  - Particularly useful for imbalanced datasets where you care more about the **minority class**.\n",
        "  - A **high area under the precision-recall curve (AUC-PR)** indicates better performance, particularly when the positive class is rare.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of Key Metrics**:\n",
        "\n",
        "| **Metric**            | **Purpose**                                            | **Best Used For**                      |\n",
        "|-----------------------|--------------------------------------------------------|----------------------------------------|\n",
        "| **Accuracy**          | Proportion of correct predictions.                     | Balanced classes.                      |\n",
        "| **Precision**         | Correctly predicted positive instances out of predicted positives. | When false positives are costly.       |\n",
        "| **Recall**            | Correctly predicted positive instances out of actual positives. | When false negatives are costly.       |\n",
        "| **F1 Score**          | Harmonic mean of Precision and Recall.                 | When Precision and Recall are equally important. |\n",
        "| **Specificity**       | Correctly predicted negatives out of actual negatives. | When focusing on the negative class.   |\n",
        "| **ROC Curve**         | Graphical representation of trade-off between TPR and FPR. | Comparing classifiers.                 |\n",
        "| **AUC-ROC**           | Area under the ROC curve.                             | Evaluating overall model performance.  |\n",
        "| **Confusion Matrix**  | Shows True Positives, True Negatives, False Positives, False Negatives. | Overall model evaluation.              |\n",
        "| **MCC**               | Measures quality of binary classification.            | Imbalanced datasets.                  |\n",
        "| **Log Loss**          | Penalizes wrong predictions with high confidence.      | Classification models outputting probabilities. |\n",
        "| **Precision-Recall Curve** | Graphical representation of Precision vs. Recall.    | Imbalanced datasets.                  |\n",
        "\n",
        "These metrics are crucial for understanding the trade-offs involved in classification problems, especially when the classes are imbalanced or when the cost of different types of errors varies."
      ],
      "metadata": {
        "id": "ZoEv3wKT602f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How does class imbalance affect Logistic Regression?"
      ],
      "metadata": {
        "id": "xjlFUpaK7Gxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Class imbalance** occurs when the number of instances in one class is significantly higher or lower than the number of instances in the other class. For example, in a binary classification problem, you might have 95% of the data belonging to class 0 and only 5% to class 1. This imbalance can negatively affect the performance of **Logistic Regression** (or any machine learning model) in several ways.\n",
        "\n",
        "Here's how **class imbalance** can affect **Logistic Regression** and what challenges it introduces:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Bias Toward the Majority Class**\n",
        "- **Problem**: Logistic Regression, like most machine learning algorithms, aims to minimize the **logistic loss (log loss)** during training. If the model is heavily exposed to the majority class, it may end up predicting the majority class for most instances to reduce the overall error. This leads to high **accuracy** but poor **performance** for the minority class.\n",
        "- **Example**: If 95% of the data belongs to class 0 and 5% to class 1, the model might predict **class 0** almost every time, as this minimizes the loss function. The model would still achieve 95% accuracy but would fail to detect the minority class (class 1).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Misleading Evaluation Metrics**\n",
        "- **Problem**: In an imbalanced dataset, traditional metrics like **accuracy** can be misleading. A model could achieve a high accuracy score simply by predicting the majority class correctly most of the time, without properly identifying the minority class.\n",
        "- **Example**: In a dataset with a 95% class 0 and 5% class 1, even if the model predicts all instances as class 0, it would have an accuracy of 95%. However, this model is not helpful for identifying class 1, which might be the more important class (e.g., fraud detection or disease diagnosis).\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Poor Prediction of Minority Class**\n",
        "- **Problem**: Logistic Regression tends to underperform for the **minority class** when the dataset is imbalanced. The model may have difficulty learning the characteristics of the minority class due to the limited number of samples and the dominance of the majority class in training.\n",
        "- **Example**: If the model only sees a few examples of the minority class during training, it may not learn the decision boundary that effectively separates the two classes, especially if the decision boundary is influenced more by the majority class.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Skewed Decision Boundary**\n",
        "- **Problem**: In an imbalanced dataset, the **decision boundary** (the threshold that separates the two classes) can be skewed towards the majority class. As a result, the model might classify a lot of instances as belonging to the majority class, which leads to a **high rate of false negatives** for the minority class.\n",
        "- **Example**: In fraud detection, the decision boundary may be biased toward non-fraudulent transactions (the majority class), meaning that fraudulent transactions (minority class) could be incorrectly classified as non-fraudulent.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Impact on Model Interpretability**\n",
        "- **Problem**: In an imbalanced dataset, the logistic regression model's **coefficients** may become biased toward the majority class, leading to misleading interpretations about which features are important.\n",
        "- **Example**: Features that are predictive of the minority class may have less influence on the model, and this could lead to incorrect or suboptimal feature selection if the model doesn't learn the minority class properly.\n",
        "\n"
      ],
      "metadata": {
        "id": "NoLIEZgs7K2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression?"
      ],
      "metadata": {
        "id": "yxnW8bfK7mPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning in Logistic Regression refers to the process of selecting the best hyperparameters to optimize the performance of the model. In machine learning, hyperparameters are the settings or configurations that are set before training the model, and their values are not learned from the data. Tuning these hyperparameters can significantly improve the model’s accuracy, precision, recall, or any other relevant evaluation metric."
      ],
      "metadata": {
        "id": "uNHl5JY17rmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used?"
      ],
      "metadata": {
        "id": "VWT6tWnM9Jwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **Logistic Regression**, the **solver** is the algorithm used to optimize the **cost function** (log-loss function) and find the best coefficients for the model. Different solvers have different characteristics in terms of speed, memory usage, and ability to handle specific types of regularization or dataset sizes.\n",
        "\n",
        "Here are the **different solvers** available in **Logistic Regression** (especially in scikit-learn):\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **liblinear**\n",
        "- **Description**: **liblinear** is an implementation of the **coordinate descent** algorithm, which is specifically designed for solving **L1 regularized** problems or when regularization is **strong** (i.e., large penalties on coefficients).\n",
        "- **Supported Regularization**: Supports **L1** and **L2** regularization.\n",
        "- **Use Case**:\n",
        "  - Best suited for small to medium-sized datasets.\n",
        "  - Preferred when you need **L1 regularization** (Lasso) because **liblinear** is one of the few solvers that handles L1 regularization well.\n",
        "  - Handles both **binary classification** and **multiclass** classification (via one-vs-rest strategy).\n",
        "- **Advantages**:\n",
        "  - Works well with smaller datasets.\n",
        "  - Works well with L1 regularization and performs feature selection.\n",
        "- **Disadvantages**:\n",
        "  - Slow on large datasets, especially when the number of samples and features is very large.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **newton-cg**\n",
        "- **Description**: The **newton-cg** solver uses the **Newton-Raphson** method for optimization. It is a second-order optimization algorithm that uses the **Hessian matrix** to perform faster convergence, especially for large datasets.\n",
        "- **Supported Regularization**: Supports only **L2 regularization** (Ridge) (doesn't support L1).\n",
        "- **Use Case**:\n",
        "  - Suitable for large datasets and when **L2 regularization** is preferred.\n",
        "  - More efficient for problems with a larger number of features.\n",
        "- **Advantages**:\n",
        "  - Faster convergence compared to **liblinear** for large datasets.\n",
        "  - Can handle **multiclass classification** efficiently.\n",
        "- **Disadvantages**:\n",
        "  - Requires more memory and may be less efficient for very sparse datasets.\n",
        "  - Doesn't support **L1 regularization**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **lbfgs**\n",
        "- **Description**: The **lbfgs** solver uses the **Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS)** algorithm. It's a quasi-Newton method that approximates the Hessian matrix, providing faster convergence than **newton-cg** with less memory.\n",
        "- **Supported Regularization**: Supports only **L2 regularization** (Ridge).\n",
        "- **Use Case**:\n",
        "  - Suitable for medium to large datasets.\n",
        "  - Best for **L2 regularization** and dense datasets.\n",
        "- **Advantages**:\n",
        "  - Very efficient in terms of memory usage.\n",
        "  - Works well with larger datasets and high-dimensional problems.\n",
        "  - Can handle **multiclass** classification.\n",
        "- **Disadvantages**:\n",
        "  - Like **newton-cg**, it doesn't support **L1 regularization**.\n",
        "  - Might be slower for very sparse datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **saga**\n",
        "- **Description**: The **saga** solver is an extension of **lbfgs** that supports **L1 regularization** (Lasso), **L2 regularization** (Ridge), and **Elastic Net** regularization. It uses a **stochastic gradient descent** (SGD)-like approach but with improved convergence properties.\n",
        "- **Supported Regularization**: Supports **L1**, **L2**, and **Elastic Net** regularization.\n",
        "- **Use Case**:\n",
        "  - Works well for large datasets, especially with **L1** regularization and **Elastic Net**.\n",
        "  - Suitable for sparse data (where most feature values are zero).\n",
        "- **Advantages**:\n",
        "  - Handles **L1 regularization** and **Elastic Net** regularization effectively.\n",
        "  - Scalable and works well with **sparse datasets** (e.g., text classification problems with sparse features).\n",
        "  - Handles **multiclass** classification.\n",
        "- **Disadvantages**:\n",
        "  - Might be slower than **liblinear** for small datasets.\n",
        "  - Requires careful tuning of learning rate and regularization.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **gd (Gradient Descent) - Not an Option in Scikit-learn**\n",
        "- Some other machine learning frameworks or models might use a basic **Gradient Descent** solver, but **scikit-learn** does not provide this directly for Logistic Regression. It is the most basic optimization algorithm where the cost function is minimized step-by-step in the direction of the negative gradient.\n",
        "\n",
        "---\n",
        "\n",
        "### **Which Solver Should You Use?**\n",
        "\n",
        "The choice of solver depends on several factors like **dataset size**, **regularization type**, and **computational resources**. Here's a guide on when to use each solver:\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Use `liblinear` when:**\n",
        "- You are working with **small to medium-sized datasets**.\n",
        "- You need **L1 regularization** (feature selection), which is supported by **liblinear**.\n",
        "- The dataset is not very sparse, and you don't need advanced solvers.\n",
        "- You are solving a **binary classification** problem (although it can handle multiclass using the one-vs-rest approach).\n",
        "\n",
        "#### **2. Use `newton-cg` or `lbfgs` when:**\n",
        "- You are working with **large datasets** (e.g., more than 10,000 samples).\n",
        "- The dataset is **dense** (few or no zero values for features).\n",
        "- You want to use **L2 regularization** (Ridge).\n",
        "- **Multiclass classification** is involved.\n",
        "- **`newton-cg`** is a good choice when memory is not a major concern, and you need speed for larger datasets.\n",
        "\n",
        "#### **3. Use `saga` when:**\n",
        "- You have a **large or sparse dataset**, such as text classification tasks.\n",
        "- You need to use **L1 regularization** (Lasso) or **Elastic Net** regularization.\n",
        "- You want the model to handle **multiclass classification**.\n",
        "- You need a solver that can efficiently handle **sparse matrices**.\n",
        "- **`saga`** is especially useful when you have very large datasets and need flexibility in regularization.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of When to Use Each Solver:**\n",
        "\n",
        "| Solver      | Regularization | Dataset Size       | Regularization Types | Notes                              |\n",
        "|-------------|----------------|--------------------|-----------------------|------------------------------------|\n",
        "| **liblinear** | L1, L2         | Small to Medium    | L1, L2                | Good for small datasets with **L1** regularization |\n",
        "| **newton-cg** | L2             | Large, Dense       | L2                    | Suitable for large, dense datasets |\n",
        "| **lbfgs**    | L2             | Medium to Large    | L2                    | Efficient, works well with dense datasets |\n",
        "| **saga**     | L1, L2, Elastic Net | Large, Sparse   | L1, L2, Elastic Net    | Efficient for sparse datasets, supports **L1** and **Elastic Net** |\n",
        "\n"
      ],
      "metadata": {
        "id": "147mbysV9OOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How is Logistic Regression extended for multiclass classification?\n"
      ],
      "metadata": {
        "id": "YPsDHbBR9jaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **Logistic Regression**, the basic model is designed for **binary classification** (i.e., predicting one of two possible outcomes). However, for **multiclass classification** (i.e., predicting one of several possible classes), **Logistic Regression** can be extended using two main approaches:\n",
        "\n",
        "1. **One-vs-Rest (OvR)** or **One-vs-All (OvA)** (most commonly used in scikit-learn)\n",
        "2. **Softmax Regression** (Multinomial Logistic Regression)\n",
        "\n",
        "Let's go over both of these extensions:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. One-vs-Rest (OvR) / One-vs-All (OvA)**\n",
        "\n",
        "**One-vs-Rest (OvR)** (also called **One-vs-All (OvA)**) is the most common way to extend logistic regression for multiclass classification. In this approach:\n",
        "\n",
        "- For **N** classes, **N** binary logistic regression classifiers are trained.\n",
        "- Each classifier is trained to distinguish one class from all the other classes. In other words, for class **i**, the classifier will try to predict:\n",
        "  - **Class i** vs. **Not Class i** (the rest of the classes).\n",
        "- When making predictions, each classifier computes the probability that a given instance belongs to its specific class. The final prediction is the class with the highest probability.\n",
        "\n",
        "#### **Steps for One-vs-Rest:**\n",
        "1. **Train** a logistic regression model for each class (i.e., one for each binary classification: \"Class i\" vs. \"Not Class i\").\n",
        "2. For a new instance, compute the **predicted probabilities** for each of the **N** classifiers.\n",
        "3. Assign the instance to the **class** with the highest predicted probability.\n",
        "\n",
        "#### **Advantages of One-vs-Rest:**\n",
        "- Simple to implement and widely used.\n",
        "- Can handle **multi-class classification** with any binary classifier, making it versatile.\n",
        "- Each class can be independently handled, and classifiers can be analyzed individually.\n",
        "\n",
        "#### **Disadvantages of One-vs-Rest:**\n",
        "- The **number of models grows linearly** with the number of classes (i.e., for N classes, you need N separate classifiers). This can be computationally expensive when dealing with many classes.\n",
        "- The approach may not always capture **dependencies** between classes, as each classifier operates independently.\n",
        "\n",
        "#### **Scikit-learn Implementation:**\n",
        "In **scikit-learn**, when you use the `LogisticRegression` class, the default approach for multiclass classification is **One-vs-Rest** (OvR). This can be seen in the parameter `multi_class`:\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create a logistic regression model\n",
        "model = LogisticRegression(multi_class='ovr')  # Default is 'ovr'\n",
        "\n",
        "# Fit the model on your training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict class labels for new data\n",
        "predictions = model.predict(X_test)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Softmax Regression (Multinomial Logistic Regression)**\n",
        "\n",
        "**Softmax Regression** (also called **Multinomial Logistic Regression**) is a more direct generalization of logistic regression to multiclass problems. Instead of training multiple binary classifiers as in **One-vs-Rest**, **Softmax Regression** computes the probability of each class directly.\n",
        "\n",
        "#### **How Softmax Regression works:**\n",
        "- In binary logistic regression, the model predicts the probability of a positive outcome, usually using the **sigmoid function**.\n",
        "- In **multiclass classification**, we use the **softmax function** to compute the probability of an instance belonging to each of the multiple classes.\n",
        "\n",
        "The **softmax function** converts the output of the logistic regression equation into probabilities for each class. If we have **K** classes, we compute a score for each class \\( z_k \\), and the softmax function normalizes these scores into probabilities:\n",
        "\n",
        "\\[\n",
        "P(y = k \\mid \\mathbf{x}) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( z_k \\) is the score (logit) for class \\( k \\), which is computed as \\( z_k = \\mathbf{x}^T \\mathbf{w}_k + b_k \\).\n",
        "- \\( e^{z_k} \\) is the exponential of the score for class \\( k \\).\n",
        "- The denominator is the sum of the exponentials of the scores for all classes, ensuring that the probabilities sum to 1.\n",
        "\n",
        "#### **Steps for Softmax Regression:**\n",
        "1. **Train** a single model that computes a score for each of the **K** classes.\n",
        "2. Apply the **softmax function** to these scores to get the class probabilities.\n",
        "3. The class with the highest probability is chosen as the predicted class.\n",
        "\n",
        "#### **Advantages of Softmax Regression:**\n",
        "- A single model is trained instead of multiple binary classifiers, which is more efficient when dealing with a large number of classes.\n",
        "- It directly captures the **interdependencies** between classes.\n",
        "- Produces a **probabilistic** output for each class, which can be useful for uncertainty estimation.\n",
        "\n",
        "#### **Disadvantages of Softmax Regression:**\n",
        "- May be computationally expensive for datasets with a very large number of classes because the softmax function involves computing the exponentials and summing over all classes.\n",
        "- The model assumes **independent classes**, and might not work well if there are strong correlations between the classes.\n",
        "\n",
        "#### **Scikit-learn Implementation:**\n",
        "In **scikit-learn**, you can use the `LogisticRegression` class with the `multi_class='multinomial'` parameter to specify the use of softmax regression:\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create a logistic regression model with softmax regression\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "\n",
        "# Fit the model on your training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict class labels for new data\n",
        "predictions = model.predict(X_test)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Differences between One-vs-Rest and Softmax Regression**\n",
        "\n",
        "| Feature                        | **One-vs-Rest**                            | **Softmax Regression**                     |\n",
        "|---------------------------------|--------------------------------------------|--------------------------------------------|\n",
        "| **Model Type**                  | Multiple binary classifiers                | A single model with multiple outputs      |\n",
        "| **Training**                    | Train one classifier per class             | Train one classifier for all classes      |\n",
        "| **Probability Calculation**     | Independent probabilities per class        | Direct calculation of class probabilities via softmax |\n",
        "| **Handling Class Relationships**| Doesn't capture relationships between classes | Naturally captures relationships between classes |\n",
        "| **Computational Cost**          | Higher (more classifiers to train)         | Lower (only one classifier)               |\n",
        "| **Preferred Use Case**          | Suitable for small to medium datasets, or when you need interpretability for each class | Suitable for larger datasets with many classes |\n"
      ],
      "metadata": {
        "id": "9-QfZb2y9qt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression?"
      ],
      "metadata": {
        "id": "zfhEflZS-Kue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression** is a popular and simple algorithm used for binary and multiclass classification tasks. Despite its simplicity, it has a number of advantages and disadvantages. Here's an overview:\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages of Logistic Regression**\n",
        "\n",
        "1. **Simplicity and Interpretability:**\n",
        "   - **Easy to implement** and understand.\n",
        "   - The output of the model (i.e., probabilities) is easy to interpret.\n",
        "   - It provides **clear insights** into the relationship between the features and the target variable, especially when working with the **coefficients** of the features.\n",
        "\n",
        "2. **Fast to Train:**\n",
        "   - **Computationally efficient** for smaller datasets and datasets with fewer features.\n",
        "   - Training time is relatively fast because it requires simple optimization (e.g., gradient descent or more sophisticated methods).\n",
        "\n",
        "3. **Works Well with Linearly Separable Data:**\n",
        "   - Performs well when the relationship between the input features and the target variable is approximately linear.\n",
        "   - For **binary classification**, it works particularly well when the data is **linearly separable**.\n",
        "\n",
        "4. **Probabilistic Output:**\n",
        "   - Outputs probabilities (between 0 and 1) that can be useful for making decisions based on the **likelihood of a class**.\n",
        "   - The model provides **probabilistic interpretation** of the predictions, allowing for **uncertainty estimation**.\n",
        "\n",
        "5. **Less Prone to Overfitting (with Regularization):**\n",
        "   - When regularization is applied (such as **L2 regularization** or **Elastic Net**), Logistic Regression can handle complex datasets without overfitting.\n",
        "   - **Regularization** helps to reduce the variance of the model, making it more robust.\n",
        "\n",
        "6. **Works Well with Linearly Related Features:**\n",
        "   - If the features are **linearly correlated** with the target, logistic regression can perform very well.\n",
        "   - For datasets where the relationships between the input features and output are linear, it will give reasonable results.\n",
        "\n",
        "7. **Multiclass Classification:**\n",
        "   - Logistic regression can be extended to **multiclass classification** using methods like **One-vs-Rest (OvR)** or **Softmax Regression (Multinomial)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Disadvantages of Logistic Regression**\n",
        "\n",
        "1. **Linear Decision Boundary:**\n",
        "   - Logistic Regression assumes that the relationship between the independent variables and the target is **linear**.\n",
        "   - This makes it **less suitable** for problems where the decision boundary is **non-linear** (for example, in the case of complex, non-linear data patterns).\n",
        "\n",
        "2. **Sensitive to Feature Scaling:**\n",
        "   - The model can be **sensitive to the scale of the features**, especially when the data has features with vastly different scales.\n",
        "   - Features with **large magnitudes** can dominate the model, so it's essential to **scale the features** (e.g., using Standardization or Normalization).\n",
        "\n",
        "3. **Not Suitable for Complex Relationships:**\n",
        "   - If the data exhibits **high complexity** or **non-linear relationships** between features and the target, **Logistic Regression** may underperform. Models like **Random Forests**, **SVM**, or **Neural Networks** are more suitable for such cases.\n",
        "\n",
        "4. **Overfitting with High Dimensionality:**\n",
        "   - Logistic Regression may overfit if there are a **large number of features** compared to the number of training samples, especially without proper **regularization**.\n",
        "   - If the number of features is too large (high-dimensional space), without enough training data, the model can easily memorize the data instead of generalizing well.\n",
        "\n",
        "5. **Requires Feature Engineering:**\n",
        "   - Logistic regression doesn't handle **raw unprocessed data** (such as raw text or images) well, so **feature engineering** is typically required to prepare the data.\n",
        "   - It may require manually adding **interaction terms** or polynomial features for better performance when the relationship between the features and the target is non-linear.\n",
        "\n",
        "6. **Assumes No Multicollinearity:**\n",
        "   - Logistic Regression assumes that the **independent variables** are not highly correlated with each other (no **multicollinearity**).\n",
        "   - If the predictors are highly correlated, the model's coefficients might become unreliable and hard to interpret.\n",
        "\n",
        "7. **Can Struggle with Imbalanced Classes:**\n",
        "   - Logistic regression can struggle when the dataset has a **severe class imbalance** (i.e., one class is much more prevalent than the other).\n",
        "   - In such cases, it may predict the **majority class** more often, leading to **poor performance on the minority class**. Techniques like **class weighting**, **oversampling**, or **undersampling** can help mitigate this issue.\n",
        "\n",
        "8. **Limited Flexibility:**\n",
        "   - Logistic regression is a **linear model**. While extensions like **polynomial regression** or **interaction terms** can add flexibility, it's still inherently less flexible than other models like **Decision Trees**, **Random Forests**, or **Neural Networks**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Table of Advantages and Disadvantages**\n",
        "\n",
        "| **Advantages**                                    | **Disadvantages**                                  |\n",
        "|---------------------------------------------------|---------------------------------------------------|\n",
        "| **Simple and interpretable model**                | Assumes linear relationships between features and target |\n",
        "| **Computationally efficient for small datasets**  | Sensitive to feature scaling                      |\n",
        "| **Works well with linearly separable data**       | Struggles with non-linear relationships           |\n",
        "| **Probabilistic output (class probabilities)**    | Requires feature engineering for complex data     |\n",
        "| **Can be regularized to avoid overfitting**       | Overfitting in high-dimensional spaces without regularization |\n",
        "| **Handles multiclass classification**             | Assumes no multicollinearity between features     |\n",
        "| **Works well with low-dimensional data**          | Can struggle with imbalanced classes              |\n",
        "\n"
      ],
      "metadata": {
        "id": "VNAiM2tl-PEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression?\n"
      ],
      "metadata": {
        "id": "Jp8u3Eq1-7Wq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression** is widely used in various fields for binary and multiclass classification tasks. It is particularly popular due to its simplicity, interpretability, and ability to output probabilities. Here are some common use cases:\n",
        "\n",
        "### **1. Medical Applications**\n",
        "   - **Disease Diagnosis:** Logistic regression is often used for predicting the likelihood of a disease based on certain features. For instance, predicting whether a patient has **diabetes**, **heart disease**, or **cancer** based on attributes like age, weight, blood pressure, cholesterol levels, etc.\n",
        "     - Example: Predicting if a person has **heart disease** (1 = Yes, 0 = No) based on features like age, cholesterol levels, and family history.\n",
        "   \n",
        "   - **Risk Assessment:** Logistic regression is used to assess the risk of adverse medical events, such as the likelihood of **stroke**, **heart attack**, or **hospital readmission**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Marketing and Customer Segmentation**\n",
        "   - **Customer Churn Prediction:** Companies use logistic regression to predict if a customer will leave or \"churn\" based on behaviors such as frequency of product use, purchase history, and customer service interactions.\n",
        "     - Example: Predicting whether a customer will cancel their subscription to a service (1 = Will churn, 0 = Will not churn).\n",
        "   \n",
        "   - **Customer Acquisition:** Logistic regression can help predict whether a potential customer will respond positively to a marketing campaign, allowing companies to target likely responders.\n",
        "     - Example: Predicting if a customer will respond to a marketing email (1 = Responds, 0 = Does not respond).\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Financial Applications**\n",
        "   - **Credit Scoring:** Logistic regression is commonly used by banks and financial institutions to predict the likelihood that a person will default on a loan based on their credit history, income, and other financial data.\n",
        "     - Example: Predicting whether a loan applicant will default on a loan (1 = Default, 0 = No default).\n",
        "   \n",
        "   - **Fraud Detection:** Logistic regression can help detect fraudulent transactions based on patterns such as transaction frequency, amounts, and the geographical location of the transaction.\n",
        "     - Example: Predicting whether a financial transaction is fraudulent (1 = Fraudulent, 0 = Legitimate).\n",
        "\n",
        "---\n",
        "\n",
        "### **4. E-commerce and Retail**\n",
        "   - **Purchase Prediction:** Logistic regression can be used to predict whether a customer will purchase a product based on browsing behavior, previous purchases, and demographics.\n",
        "     - Example: Predicting whether a customer will buy a product after viewing it online (1 = Purchase, 0 = No purchase).\n",
        "   \n",
        "   - **Product Recommendations:** Logistic regression can help in recommending products to customers by predicting the likelihood of a customer being interested in a product based on their past browsing and purchase behavior.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Social Media and Sentiment Analysis**\n",
        "   - **Sentiment Classification:** Logistic regression is often applied to sentiment analysis tasks to classify text into positive or negative sentiments. This can be used to analyze social media posts, customer reviews, or feedback on products or services.\n",
        "     - Example: Classifying a tweet as either **positive** or **negative** sentiment (1 = Positive, 0 = Negative).\n",
        "   \n",
        "   - **Spam Detection:** Logistic regression can be used to classify emails or messages as spam or not spam based on features such as the content of the message, sender, and subject line.\n",
        "     - Example: Predicting whether an email is spam (1 = Spam, 0 = Not Spam).\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Human Resources**\n",
        "   - **Employee Retention:** Companies can use logistic regression to predict whether an employee is likely to leave the organization based on factors such as performance, tenure, salary, and satisfaction levels.\n",
        "     - Example: Predicting whether an employee will leave the company (1 = Will leave, 0 = Will stay).\n",
        "   \n",
        "   - **Hiring and Recruitment:** Logistic regression can be used to predict whether a job applicant is likely to be hired based on their qualifications, previous experience, and interview performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Sports Analytics**\n",
        "   - **Match Outcome Prediction:** Logistic regression can be used to predict the outcome of a sports match (win/loss) based on features such as player statistics, team rankings, and past performance.\n",
        "     - Example: Predicting whether a football team will win a match based on past team performance metrics (1 = Win, 0 = Loss).\n",
        "   \n",
        "   - **Player Performance:** It can also be used to predict whether a player will perform well in a given game based on historical data and conditions.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Political Science and Voting Behavior**\n",
        "   - **Election Outcome Prediction:** Logistic regression is used to predict voting behavior based on features such as age, gender, location, and political affiliation.\n",
        "     - Example: Predicting whether a voter will vote for a particular candidate (1 = Vote for Candidate A, 0 = Vote for Candidate B).\n",
        "   \n",
        "   - **Voter Turnout Prediction:** It can be used to predict the likelihood that a person will vote in an election based on factors like past voting history, age, and education level.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. Image and Text Classification**\n",
        "   - **Document Classification:** Logistic regression can be used for classifying text documents into categories (e.g., news articles into topics like politics, sports, technology, etc.).\n",
        "     - Example: Classifying news articles as either **politics** or **not politics** (1 = Politics, 0 = Not Politics).\n",
        "   \n",
        "   - **Image Classification (Binary):** Logistic regression can also be used for simple binary image classification tasks, such as detecting whether an image contains a specific object or not (e.g., classifying whether an image contains a **cat** or **not**).\n",
        "\n",
        "---\n",
        "\n",
        "### **10. Healthcare Predictive Modeling**\n",
        "   - **Predicting Patient Outcomes:** Logistic regression can predict whether a patient is likely to recover from an illness or disease based on various health metrics, treatments, and other factors.\n",
        "     - Example: Predicting if a patient will recover after a surgery (1 = Recover, 0 = Not Recover).\n",
        "   \n",
        "   - **Readmission Risk:** It can be used to predict the likelihood of a patient being readmitted to the hospital after discharge based on factors like age, medical history, and hospital stay.\n",
        "\n",
        "---\n",
        "\n",
        "### **11. Natural Language Processing (NLP)**\n",
        "   - **Text Classification:** Logistic regression can be used for text classification tasks such as determining if a document is related to a specific topic (e.g., spam detection, sentiment analysis).\n",
        "   - **Named Entity Recognition (NER):** Logistic regression can help identify and classify named entities (such as people, locations, and organizations) in a text.\n",
        "\n",
        "---\n",
        "\n",
        "### **12. Web Analytics and Click-Through Rate (CTR) Prediction**\n",
        "   - **Ad Click Prediction:** Logistic regression is commonly used to predict whether a user will click on an online advertisement based on features such as the user's browsing behavior, the context of the ad, and user demographics.\n",
        "     - Example: Predicting whether a user will click on a specific ad (1 = Click, 0 = No click).\n",
        "\n"
      ],
      "metadata": {
        "id": "Q9hw5mue_R6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression?"
      ],
      "metadata": {
        "id": "O3meZceV_8-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression** and **Softmax Regression** (also known as **Multinomial Logistic Regression**) are both used for classification tasks, but they are applied in different scenarios. Below is a comparison of the two:\n",
        "\n",
        "### **1. Purpose / Type of Classification**\n",
        "\n",
        "- **Logistic Regression (Binary Classification)**:\n",
        "  - **Logistic Regression** is used for **binary classification** tasks. It predicts the probability that a given input belongs to one of two classes (often labeled as 0 and 1).\n",
        "  - Example: Predicting whether an email is **spam (1)** or **not spam (0)**.\n",
        "  \n",
        "- **Softmax Regression (Multiclass Classification)**:\n",
        "  - **Softmax Regression** is used for **multiclass classification** tasks, where there are more than two possible classes to predict. It generalizes logistic regression to handle multiple classes.\n",
        "  - Example: Predicting the **species of a flower** (e.g., Setosa, Versicolor, Virginica) based on features like petal length, width, etc.\n",
        "\n",
        "### **2. Output**\n",
        "\n",
        "- **Logistic Regression**:\n",
        "  - The output is a **single probability** for the positive class (1) using the **sigmoid function**. The probability for the negative class (0) is the complement of the positive class probability (i.e., 1 - P).\n",
        "  - The prediction is made by thresholding the probability: If P > 0.5, predict class 1; otherwise, predict class 0.\n",
        "  - The equation for logistic regression is:\n",
        "    \\[\n",
        "    P(y=1|X) = \\frac{1}{1 + e^{-(w^T X + b)}}\n",
        "    \\]\n",
        "  \n",
        "- **Softmax Regression**:\n",
        "  - The output is a **vector of probabilities**, one for each class. Softmax ensures that the sum of the probabilities across all classes is 1.\n",
        "  - The prediction is made by selecting the class with the highest probability.\n",
        "  - The equation for the softmax function for a class \\( k \\) is:\n",
        "    \\[\n",
        "    P(y=k|X) = \\frac{e^{w_k^T X + b_k}}{\\sum_{j=1}^{K} e^{w_j^T X + b_j}}\n",
        "    \\]\n",
        "    where \\( K \\) is the number of classes, and the denominator ensures that the probabilities across all classes sum to 1.\n",
        "\n",
        "### **3. Model Setup**\n",
        "\n",
        "- **Logistic Regression**:\n",
        "  - In binary logistic regression, you only have a **single set of weights** (one for each feature), and the model learns a single **decision boundary** between the two classes.\n",
        "  - The model uses the **sigmoid function** to model the output as a probability for one of two classes.\n",
        "\n",
        "- **Softmax Regression**:\n",
        "  - In softmax regression, there are **multiple sets of weights**, one for each class. Each class has its own set of coefficients, and the model computes a separate linear combination for each class.\n",
        "  - The output is a **probability distribution** over all classes, with the **highest probability** indicating the predicted class.\n",
        "\n",
        "### **4. Cost Function (Loss Function)**\n",
        "\n",
        "- **Logistic Regression**:\n",
        "  - The cost function used is the **Binary Cross-Entropy loss (Log Loss)**, which is suitable for binary classification:\n",
        "    \\[\n",
        "    J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
        "    \\]\n",
        "    where \\( h_\\theta(x) \\) is the predicted probability for class 1.\n",
        "\n",
        "- **Softmax Regression**:\n",
        "  - The cost function used is the **Categorical Cross-Entropy loss** (also called **Multinomial Log Loss**), which is an extension of binary cross-entropy to multiple classes:\n",
        "    \\[\n",
        "    J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log(P(y_k^{(i)} | x^{(i)}))\n",
        "    \\]\n",
        "    where \\( K \\) is the number of classes, and \\( y_k^{(i)} \\) is the indicator (0 or 1) for whether class \\( k \\) is the true class for the \\(i\\)-th example.\n",
        "\n",
        "### **5. Decision Boundary**\n",
        "\n",
        "- **Logistic Regression**:\n",
        "  - In **binary logistic regression**, the decision boundary is a **single hyperplane** in the feature space that separates the two classes.\n",
        "  \n",
        "- **Softmax Regression**:\n",
        "  - In **softmax regression**, there are **multiple decision boundaries**. The model uses a set of linear decision boundaries (one for each class), and the class with the highest probability is selected as the predicted class.\n",
        "\n",
        "### **6. Example of Use Cases**\n",
        "\n",
        "- **Logistic Regression**:\n",
        "  - **Medical Diagnosis**: Predicting if a patient has a certain disease (e.g., cancer detection).\n",
        "  - **Fraud Detection**: Predicting whether a transaction is fraudulent (1 = Fraud, 0 = Legitimate).\n",
        "  - **Spam Classification**: Predicting if an email is spam (1 = Spam, 0 = Not Spam).\n",
        "\n",
        "- **Softmax Regression**:\n",
        "  - **Handwritten Digit Recognition**: Predicting the digit (0-9) from an image of handwritten digits (multiclass).\n",
        "  - **Image Classification**: Predicting the class of an object in an image (e.g., cat, dog, bird, etc.).\n",
        "  - **Customer Segmentation**: Classifying customers into multiple groups based on behavior (e.g., high value, medium value, low value).\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of Differences**\n",
        "\n",
        "| **Feature**                   | **Logistic Regression**                        | **Softmax Regression**                     |\n",
        "|-------------------------------|-----------------------------------------------|--------------------------------------------|\n",
        "| **Type of Classification**     | Binary Classification                         | Multiclass Classification                  |\n",
        "| **Output**                     | Single probability (for one class)            | Probability distribution over all classes  |\n",
        "| **Decision Boundary**          | Single decision boundary (hyperplane)         | Multiple decision boundaries (hyperplanes) |\n",
        "| **Cost Function**              | Binary Cross-Entropy (Log Loss)               | Categorical Cross-Entropy (Multinomial Log Loss) |\n",
        "| **Solver**                     | Sigmoid function                               | Softmax function                          |\n",
        "| **Use Case**                   | Binary outcomes (0 or 1)                      | Multiple classes (more than two outcomes)  |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AHD0JhKVDsSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?"
      ],
      "metadata": {
        "id": "VJutIusRDyJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When choosing between **One-vs-Rest (OvR)** and **Softmax** (also known as **Multinomial Logistic Regression**) for multiclass classification, it’s important to understand the key differences between these approaches and consider the specifics of your problem. Here's a comparison of both methods and factors to help you decide which one to use.\n",
        "\n",
        "### **1. One-vs-Rest (OvR) Approach:**\n",
        "\n",
        "#### **Overview:**\n",
        "- In the **OvR** approach, you train a **binary classifier** for each class. The model treats each class as a **binary classification problem** by distinguishing the class from all other classes.\n",
        "- For a problem with \\(K\\) classes, you will train \\(K\\) separate classifiers, each one predicting whether an input belongs to a particular class or not.\n",
        "- After training the classifiers, you predict the class by choosing the classifier with the highest predicted probability.\n",
        "\n",
        "#### **How It Works:**\n",
        "- For each class, you train a classifier that outputs:\n",
        "  \\[\n",
        "  P(\\text{class}_k | X) = \\frac{1}{1 + e^{-(w_k^T X + b_k)}}\n",
        "  \\]\n",
        "  This is essentially a logistic regression applied to each class.\n",
        "- At prediction time, each classifier provides a probability, and the class with the highest probability is selected as the final prediction.\n",
        "\n",
        "#### **Advantages of OvR:**\n",
        "- **Scalability**: Since OvR trains one classifier per class, it is suitable for situations where the number of classes \\(K\\) is large, and it can be easier to scale with many classes.\n",
        "- **Flexibility**: You can use any binary classifier in the OvR approach, such as logistic regression, SVM, or decision trees.\n",
        "- **Interpretability**: It’s easier to interpret the decision-making of individual classifiers.\n",
        "  \n",
        "#### **Disadvantages of OvR:**\n",
        "- **Inefficiency**: You need to train \\(K\\) classifiers, which can be computationally expensive, especially with large numbers of classes.\n",
        "- **Class Imbalance**: In some cases, the classifiers might be biased towards the majority class when there is a significant imbalance between classes.\n",
        "- **Correlation Between Classes**: OvR treats each class independently, ignoring correlations between classes. It doesn’t model the relationships between classes, which might be important in some problems.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Softmax (Multinomial Logistic Regression) Approach:**\n",
        "\n",
        "#### **Overview:**\n",
        "- The **Softmax** approach (also called **Multinomial Logistic Regression**) is a **single classifier** that directly predicts probabilities for all classes at once.\n",
        "- This method generalizes logistic regression to handle multiple classes by using the **softmax function** to compute a probability distribution across all possible classes.\n",
        "\n",
        "#### **How It Works:**\n",
        "- The model computes a set of logits (one for each class) based on the input features:\n",
        "  \\[\n",
        "  \\mathbf{z}_k = w_k^T X + b_k \\quad \\text{for each class } k\n",
        "  \\]\n",
        "  Then, the softmax function is applied to these logits to get the probabilities:\n",
        "  \\[\n",
        "  P(y=k | X) = \\frac{e^{\\mathbf{z}_k}}{\\sum_{j=1}^{K} e^{\\mathbf{z}_j}}\n",
        "  \\]\n",
        "  This gives a probability distribution where the sum of probabilities across all classes equals 1.\n",
        "- At prediction time, the class with the highest probability is selected.\n",
        "\n",
        "#### **Advantages of Softmax:**\n",
        "- **Joint Optimization**: Softmax models all classes jointly, which allows the model to learn better when there are **interdependencies** between classes. It is particularly useful when classes are not independent and may exhibit correlations.\n",
        "- **Single Model**: Only one classifier is trained, which is computationally more efficient compared to training \\(K\\) separate classifiers in the OvR approach.\n",
        "- **Probability Distribution**: Softmax provides a full **probability distribution** across all classes, not just the predicted class. This can be valuable if you need uncertainty estimates for each prediction.\n",
        "\n",
        "#### **Disadvantages of Softmax:**\n",
        "- **Computational Cost**: While you only need one classifier, the softmax function requires calculating probabilities for all classes in each prediction, which can be computationally intensive if there are many classes.\n",
        "- **Not as flexible**: The softmax approach requires a **single model** and may not be as flexible as OvR when trying to use different algorithms (e.g., SVM) for each class.\n",
        "\n"
      ],
      "metadata": {
        "id": "KDYpGNWiD6VT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?\n"
      ],
      "metadata": {
        "id": "aLWu4jVIEUfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **Logistic Regression**, the coefficients (also known as the weights) have specific interpretations, but they differ from those in **Linear Regression** due to the non-linear nature of the logistic function (sigmoid). Here's how to interpret the coefficients in Logistic Regression:\n",
        "\n",
        "### 1. **Log-Odds and Coefficients**\n",
        "In logistic regression, the model predicts the **log-odds** of the outcome. The logistic regression equation is:\n",
        "\n",
        "\\[\n",
        "\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( p \\) is the probability of the event (e.g., \\( y = 1 \\)).\n",
        "- \\( \\beta_0 \\) is the intercept (bias term).\n",
        "- \\( \\beta_1, \\beta_2, \\dots, \\beta_n \\) are the coefficients (weights) for each feature \\( x_1, x_2, \\dots, x_n \\).\n",
        "\n",
        "The **logit function** transforms the output from the linear combination of features (like in linear regression) into a probability range between 0 and 1, using the sigmoid function.\n",
        "\n",
        "### 2. **Interpretation of Coefficients**\n",
        "Each coefficient (\\( \\beta_j \\)) represents the change in the **log-odds** of the dependent variable \\( y \\) for a one-unit change in the corresponding feature \\( x_j \\), while keeping all other features constant.\n",
        "\n",
        "- **For a given feature \\( x_j \\):**  \n",
        "  The coefficient \\( \\beta_j \\) indicates how much the **log-odds** of the outcome increase or decrease when \\( x_j \\) increases by 1 unit.\n",
        "  - If \\( \\beta_j > 0 \\), the log-odds (and thus the probability) of the event occurring increase as \\( x_j \\) increases.\n",
        "  - If \\( \\beta_j < 0 \\), the log-odds (and thus the probability) of the event occurring decrease as \\( x_j \\) increases.\n",
        "\n",
        "### 3. **Odds Ratio**\n",
        "To interpret the effect of a predictor in terms of odds, we can exponentiate the coefficient \\( \\beta_j \\) (i.e., compute \\( e^{\\beta_j} \\)):\n",
        "\n",
        "\\[\n",
        "\\text{Odds Ratio} = e^{\\beta_j}\n",
        "\\]\n",
        "\n",
        "- The **odds ratio** represents how the odds of the event happening (versus not happening) change with a one-unit increase in \\( x_j \\), while holding other features constant.\n",
        "  - If \\( \\beta_j > 0 \\), \\( e^{\\beta_j} > 1 \\), which means the odds of the event happening increase as \\( x_j \\) increases.\n",
        "  - If \\( \\beta_j < 0 \\), \\( e^{\\beta_j} < 1 \\), which means the odds of the event happening decrease as \\( x_j \\) increases.\n",
        "  - If \\( \\beta_j = 0 \\), \\( e^{\\beta_j} = 1 \\), meaning that \\( x_j \\) has no effect on the odds of the event.\n",
        "\n",
        "### **Example:**\n",
        "Let's consider a logistic regression model predicting whether a student passes or fails an exam (pass = 1, fail = 0). Suppose the model is:\n",
        "\n",
        "\\[\n",
        "\\text{logit}(p) = -2 + 0.05 \\times (\\text{study hours}) + 0.3 \\times (\\text{previous score})\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\beta_0 = -2 \\) is the intercept (bias term).\n",
        "- \\( \\beta_1 = 0.05 \\) is the coefficient for \"study hours\".\n",
        "- \\( \\beta_2 = 0.3 \\) is the coefficient for \"previous score\".\n",
        "\n",
        "#### **Interpretation:**\n",
        "- **Intercept (\\( \\beta_0 = -2 \\))**:  \n",
        "  When both study hours and previous score are 0, the log-odds of passing the exam are -2. In other words, the odds of passing the exam when the student has 0 study hours and 0 previous score is \\( e^{-2} \\).\n",
        "  \n",
        "- **Study Hours (\\( \\beta_1 = 0.05 \\))**:  \n",
        "  For each additional hour of study, the **log-odds of passing** the exam increase by 0.05, keeping the previous score constant.\n",
        "  - The **odds ratio** for study hours is \\( e^{0.05} \\approx 1.051 \\), meaning that for each additional hour of study, the odds of passing the exam increase by approximately 5.1%.\n",
        "\n",
        "- **Previous Score (\\( \\beta_2 = 0.3 \\))**:  \n",
        "  For each one-point increase in the previous score, the **log-odds of passing** the exam increase by 0.3, keeping the study hours constant.\n",
        "  - The **odds ratio** for previous score is \\( e^{0.3} \\approx 1.35 \\), meaning that for each additional point in the previous score, the odds of passing the exam increase by 35%.\n",
        "\n",
        "### 4. **Example in Context**\n",
        "If you want to calculate the **probability of passing the exam** for a student with 5 hours of study and a previous score of 80, we first compute the log-odds using the logistic regression equation:\n",
        "\n",
        "\\[\n",
        "\\text{logit}(p) = -2 + 0.05 \\times 5 + 0.3 \\times 80 = -2 + 0.25 + 24 = 22.25\n",
        "\\]\n",
        "\n",
        "Now, convert the log-odds to a probability using the sigmoid function:\n",
        "\n",
        "\\[\n",
        "p = \\frac{1}{1 + e^{-22.25}} \\approx 1\n",
        "\\]\n",
        "\n",
        "So, the probability of passing the exam for this student is very close to 1 (or 100%).\n",
        "\n",
        "### **5. Summary of Interpretation**\n",
        "- **Coefficients (\\( \\beta_j \\))**: The change in **log-odds** for a one-unit change in \\( x_j \\).\n",
        "- **Odds Ratio**: \\( e^{\\beta_j} \\) indicates how the odds change with a one-unit change in \\( x_j \\).\n",
        "- **Sign of Coefficients**:\n",
        "  - Positive \\( \\beta_j \\) means higher odds of the event (increasing \\( x_j \\) increases the probability of the event).\n",
        "  - Negative \\( \\beta_j \\) means lower odds of the event (increasing \\( x_j \\) decreases the probability of the event).\n",
        "\n",
        "In practice, it's common to report the **odds ratio** \\( e^{\\beta_j} \\) for each predictor to make the interpretation more intuitive, as it directly tells how much the odds of the event change with a one-unit increase in the predictor."
      ],
      "metadata": {
        "id": "tI0y1kqMEaBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical"
      ],
      "metadata": {
        "id": "mmLseu_MEt0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy."
      ],
      "metadata": {
        "id": "dPjXwSj-E27N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiMh18a1FBON",
        "outputId": "d426d9b6-340a-41b9-cfe4-72673f1c4c28"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. C Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy."
      ],
      "metadata": {
        "id": "sfAZxYhZFUQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy (with L1 regularization): {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQWbf0xnFY_M",
        "outputId": "6419cf50-43e2-4a0c-8ff6-e954c82d77a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (with L1 regularization): 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients.**bold text**"
      ],
      "metadata": {
        "id": "cKMXxKRVFjK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Iris dataset for example)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Convert the problem to a binary classification problem (for simplicity)\n",
        "# We will classify only if the flower is of class 0 (setosa) or not\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the model coefficients (weights for each feature)\n",
        "print(\"Model Coefficients (Weights):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAAHO_yZFyKd",
        "outputId": "6045335e-e366-4724-9ad8-f870073d0137"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n",
            "Model Coefficients (Weights):\n",
            "[[ 0.36479402  1.35499766 -2.09628559 -0.92154751]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')?\n"
      ],
      "metadata": {
        "id": "hyg65dTIF_50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Iris dataset for example)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Convert the problem to a binary classification problem (for simplicity)\n",
        "# We will classify only if the flower is of class 0 (setosa) or not\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with Elastic Net regularization\n",
        "# We need to define the l1_ratio, which controls the mix between L1 and L2 regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the model coefficients (weights for each feature)\n",
        "print(\"Model Coefficients (Weights):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McL2hC4hFoTm",
        "outputId": "11ff887a-e6d2-4d58-a946-94faf7536e5b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n",
            "Model Coefficients (Weights):\n",
            "[[ 0.25483238  1.49459686 -2.24466371 -0.71696944]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'."
      ],
      "metadata": {
        "id": "1R8ZlYPGGSAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model for multiclass classification with One-vs-Rest strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print the model coefficients (weights for each feature)\n",
        "print(\"Model Coefficients (Weights):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9AhwcrtGXwz",
        "outputId": "a6be8a13-68ff-408a-d776-7afb73642298"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 97.78%\n",
            "Model Coefficients (Weights):\n",
            "[[ 0.36479402  1.35499766 -2.09628559 -0.92154751]\n",
            " [ 0.4808915  -1.58463288  0.3937527  -1.09224057]\n",
            " [-1.5286415  -1.43244729  2.3048277   2.08584535]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "nNNKnovDGho_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],       # Type of regularization\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Train the model using GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters from GridSearchCV\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Best Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LZORzvPGnx3",
        "outputId": "d5917f9a-1430-44b2-bc1b-cb07f3fef6a0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 1, 'penalty': 'l2'}\n",
            "Best Model Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "25 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.86666667        nan 0.93333333        nan 0.96190476\n",
            "        nan 0.94285714        nan 0.95238095]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy."
      ],
      "metadata": {
        "id": "b1iuReBsGyt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Set up Stratified K-Fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and calculate accuracy scores for each fold\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "print(f\"Accuracy for each fold: {scores}\")\n",
        "\n",
        "# Calculate and print the average accuracy\n",
        "average_accuracy = np.mean(scores)\n",
        "print(f\"Average Accuracy: {average_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDuKZYg-G4mS",
        "outputId": "a990c553-795a-4e29-c20c-258c7ad4d837"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for each fold: [1.         0.96666667 0.93333333 1.         0.93333333]\n",
            "Average Accuracy: 96.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy."
      ],
      "metadata": {
        "id": "r7yyLz_8HAkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris  # Import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure\n",
        "print(\"Dataset head:\")\n",
        "print(df.head())\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop(columns=['target'])  # Features (all columns except 'target')\n",
        "y = df['target']  # Target variable (the column containing labels)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzhDX1VRHGAO",
        "outputId": "e5d9e894-a976-4f94-a8ae-bb8a6bce2474"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset head:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n",
            "Model Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "P9FukDpRHPBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': uniform(0.1, 10),  # Random values for C between 0.1 and 10\n",
        "    'penalty': ['l1', 'l2'],  # Either L1 (Lasso) or L2 (Ridge) penalty\n",
        "    'solver': ['liblinear', 'saga']  # Solvers that support L1 regularization\n",
        "}\n",
        "\n",
        "# Set up RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=100,  # Number of random combinations to try\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    scoring='accuracy',  # We want to maximize accuracy\n",
        "    random_state=42,  # For reproducibility\n",
        "    n_jobs=-1  # Use all available CPU cores\n",
        ")\n",
        "\n",
        "# Perform the randomized search\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters from RandomizedSearchCV\n",
        "best_params = random_search.best_params_\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "# Predict the labels for the test set using the best model\n",
        "y_pred = random_search.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Best Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUUSjexgHpJ-",
        "outputId": "8b58e22c-2866-4ca6-9cf1-6fa4752938c3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 8.424426408004217, 'penalty': 'l2', 'solver': 'saga'}\n",
            "Best Model Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy."
      ],
      "metadata": {
        "id": "8kuytalvH04V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "log_reg_model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Create a OneVsOneClassifier with Logistic Regression as the base classifier\n",
        "ovo_model = OneVsOneClassifier(log_reg_model)\n",
        "\n",
        "# Train the model\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"One-vs-One Multiclass Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58ohxgv3H8ml",
        "outputId": "18dbfd3a-a4ed-41c8-fa52-af783872f52c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Multiclass Logistic Regression Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification."
      ],
      "metadata": {
        "id": "VOFwKc6lIIoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load the breast cancer dataset (binary classification problem)\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n",
        "plt.title('Confusion Matrix for Logistic Regression')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "Sp4bLoF6IMpF",
        "outputId": "567a4c48-f2a4-4842-af07-a7164c55157a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 95.61%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score."
      ],
      "metadata": {
        "id": "GGxkcvzrIZi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Load the breast cancer dataset (binary classification problem)\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"F1-Score: {f1 * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_6pgoecIf2k",
        "outputId": "e32597e6-829c-4cbd-dd55-6a36d9b8d4c7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 95.61%\n",
            "Precision: 94.59%\n",
            "Recall: 98.59%\n",
            "F1-Score: 96.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance."
      ],
      "metadata": {
        "id": "B0FS4tPnIpP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load the breast cancer dataset (binary classification problem)\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Introduce imbalance: we'll downsample the majority class\n",
        "# Identify indices for each class\n",
        "class_0_indices = np.where(y == 0)[0]\n",
        "class_1_indices = np.where(y == 1)[0]\n",
        "\n",
        "# Resample class 0 to make the data imbalanced, ensuring it's smaller than class 1\n",
        "# n_samples is now less than the length of class_0_indices. We set it to half the number of class_1_indices, which makes class 0 less numerous.\n",
        "class_0_undersampled = resample(class_0_indices, replace=False, n_samples=len(class_1_indices) // 2, random_state=42) # added a random_state for consistency.\n",
        "\n",
        "# Create the imbalanced dataset\n",
        "imbalanced_indices = np.concatenate([class_0_undersampled, class_1_indices])\n",
        "X_imbalanced = X[imbalanced_indices]\n",
        "y_imbalanced = y[imbalanced_indices]\n",
        "\n",
        "# Split the imbalanced dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_imbalanced, y_imbalanced, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with class weights balanced\n",
        "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy and classification report (Precision, Recall, F1-Score)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\\n\", classification_rep)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl1GaDrZItE-",
        "outputId": "19507bf2-b3f2-47a8-a7c5-d279046c2f33"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 93.46%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92        44\n",
            "           1       0.95      0.94      0.94        63\n",
            "\n",
            "    accuracy                           0.93       107\n",
            "   macro avg       0.93      0.93      0.93       107\n",
            "weighted avg       0.93      0.93      0.93       107\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance."
      ],
      "metadata": {
        "id": "I1pAhSCEJuAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Import necessary modules\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load the Titanic dataset from Kaggle.\n",
        "# The data set is available at: https://www.kaggle.com/c/titanic/data\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv' # This is the raw CSV URL from github\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Check for missing values and print their counts\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Fill missing 'Age' with the median value of the column\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "\n",
        "# Fill missing 'Embarked' with the mode (most frequent) value\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Change Pclass to categorical type\n",
        "data['Pclass'] = data['Pclass'].astype('category')\n",
        "\n",
        "# Encode categorical features (e.g., 'Sex', 'Embarked')\n",
        "data = pd.get_dummies(data, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\n",
        "\n",
        "# Drop columns that won't be used for the model (e.g., 'Name', 'Ticket', 'Cabin', 'PassengerId')\n",
        "data.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'], inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(columns='Survived')  # All columns except 'Survived' are features\n",
        "y = data['Survived']  # The target variable is 'Survived'\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model performance using multiple metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print performance metrics\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"F1-Score: {f1 * 100:.2f}%\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKlqHTHTLYnf",
        "outputId": "0edc06ce-a11c-4dc3-d870-4a5b8720c260"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n",
            "PassengerId      0\n",
            "Survived         0\n",
            "Pclass           0\n",
            "Name             0\n",
            "Sex              0\n",
            "Age            177\n",
            "SibSp            0\n",
            "Parch            0\n",
            "Ticket           0\n",
            "Fare             0\n",
            "Cabin          687\n",
            "Embarked         2\n",
            "dtype: int64\n",
            "Model Accuracy: 79.89%\n",
            "Precision: 77.94%\n",
            "Recall: 71.62%\n",
            "F1-Score: 74.65%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.83       105\n",
            "           1       0.78      0.72      0.75        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.79      0.79       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling."
      ],
      "metadata": {
        "id": "98vBCejrMtSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load the Titanic dataset and perform preprocessing.\n",
        "# This is performed once to avoid redundant processing in subsequent code cells.\n",
        "url_titanic = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'  # Updated URL\n",
        "data_titanic = pd.read_csv(url_titanic)\n",
        "\n",
        "# Check for missing values and print their counts\n",
        "print(\"Titanic Dataset Missing Values Before Processing\")\n",
        "print(data_titanic.isnull().sum())\n",
        "\n",
        "# Fill missing 'Age' with the median value of the column\n",
        "data_titanic['Age'].fillna(data_titanic['Age'].median(), inplace=True)\n",
        "\n",
        "# Fill missing 'Embarked' with the mode (most frequent) value\n",
        "data_titanic['Embarked'].fillna(data_titanic['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Change Pclass to categorical type\n",
        "data_titanic['Pclass'] = data_titanic['Pclass'].astype('category')\n",
        "\n",
        "# Encode categorical features (e.g., 'Sex', 'Embarked')\n",
        "data_titanic = pd.get_dummies(data_titanic, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\n",
        "\n",
        "# Drop columns that won't be used for the model (e.g., 'Name', 'Ticket', 'Cabin', 'PassengerId')\n",
        "data_titanic.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'], inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X_titanic = data_titanic.drop(columns='Survived')  # All columns except 'Survived' are features\n",
        "y_titanic = data_titanic['Survived']  # The target variable is 'Survived'\n",
        "\n",
        "# Load the breast cancer dataset (binary classification problem)\n",
        "data_cancer = load_breast_cancer()\n",
        "X_cancer = data_cancer.data\n",
        "y_cancer = data_cancer.target\n",
        "\n",
        "# Introduce imbalance: we'll downsample the majority class\n",
        "# Identify indices for each class\n",
        "class_0_indices = np.where(y_cancer == 0)[0]\n",
        "class_1_indices = np.where(y_cancer == 1)[0]\n",
        "\n",
        "# Resample class 0 to make the data imbalanced, ensuring it's smaller than class 1\n",
        "# n_samples is now less than the length of class_0_indices. We set it to half the number of class_1_indices, which makes class 0 less numerous.\n",
        "class_0_undersampled = resample(class_0_indices, replace=False, n_samples=len(class_1_indices) // 2, random_state=42)  # added a random_state for consistency.\n",
        "\n",
        "# Create the imbalanced dataset\n",
        "imbalanced_indices = np.concatenate([class_0_undersampled, class_1_indices])\n",
        "X_imbalanced = X_cancer[imbalanced_indices]\n",
        "y_imbalanced = y_cancer[imbalanced_indices]\n",
        "\n",
        "# Split the imbalanced dataset into training and testing sets (80% train, 20% test)\n",
        "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(X_imbalanced, y_imbalanced, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with class weights balanced\n",
        "model_cancer = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "\n",
        "# Train the model\n",
        "model_cancer.fit(X_train_cancer, y_train_cancer)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_cancer = model_cancer.predict(X_test_cancer)\n",
        "\n",
        "# Evaluate the model performance\n",
        "accuracy = accuracy_score(y_test_cancer, y_pred_cancer)\n",
        "classification_rep = classification_report(y_test_cancer, y_pred_cancer)\n",
        "\n",
        "# Print the accuracy and classification report (Precision, Recall, F1-Score)\n",
        "print(f\"Cancer Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Cancer Classification Report:\\n\", classification_rep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7t5n38xTNA_R",
        "outputId": "f1e8cfbb-89c7-4842-fdb7-c63a1f09614d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titanic Dataset Missing Values Before Processing\n",
            "PassengerId      0\n",
            "Survived         0\n",
            "Pclass           0\n",
            "Name             0\n",
            "Sex              0\n",
            "Age            177\n",
            "SibSp            0\n",
            "Parch            0\n",
            "Ticket           0\n",
            "Fare             0\n",
            "Cabin          687\n",
            "Embarked         2\n",
            "dtype: int64\n",
            "Cancer Model Accuracy: 93.46%\n",
            "Cancer Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92        44\n",
            "           1       0.95      0.94      0.94        63\n",
            "\n",
            "    accuracy                           0.93       107\n",
            "   macro avg       0.93      0.93      0.93       107\n",
            "weighted avg       0.93      0.93      0.93       107\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score."
      ],
      "metadata": {
        "id": "M9nYtBJQNU48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Titanic dataset (you can replace the URL with your local dataset path)\n",
        "# Note this dataframe is from a different URL than the previous files and does not contain the column 'Embarked'\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "#data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True) # Embarked column does not exist.\n",
        "data.dropna(subset=['Fare'], inplace=True)\n",
        "\n",
        "# Encode categorical features (e.g., 'Sex', 'Pclass')\n",
        "data = pd.get_dummies(data, columns=['Sex','Pclass'], drop_first=True)\n",
        "\n",
        "# Drop columns that won't be used for the model.\n",
        "# These columns are not present in this dataset.\n",
        "#data.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'], inplace=True)\n",
        "data.drop(columns=['Name'], inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(columns='Survived')  # All columns except 'Survived' are features\n",
        "y = data['Survived']  # The target variable is 'Survived'\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling (Standardization) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get predicted probabilities (needed for ROC-AUC score)\n",
        "y_pred_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "print(f\"ROC-AUC score: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0goyC-lPNcDT",
        "outputId": "8aee00a0-f4f8-458a-c2a4-d47836054c08"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC score: 0.8169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy."
      ],
      "metadata": {
        "id": "XN2sxkMlODtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Titanic dataset (you can replace the URL with your local dataset path)\n",
        "# Updated to use the same dataset with Embarked column as in ipython-input-20-e7e00c7a3ce7\n",
        "url_titanic = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "data = pd.read_csv(url_titanic)\n",
        "\n",
        "# Handle missing values\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "data.dropna(subset=['Fare'], inplace=True)\n",
        "\n",
        "# Encode categorical features (e.g., 'Sex', 'Embarked', 'Pclass')\n",
        "data = pd.get_dummies(data, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\n",
        "\n",
        "# Drop columns that won't be used for the model\n",
        "data.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'], inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(columns='Survived')  # All columns except 'Survived' are features\n",
        "y = data['Survived']  # The target variable is 'Survived'\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling (Standardization) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train a Logistic Regression model with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with C=0.5: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUOfblPrTKr3",
        "outputId": "3fe4b542-6428-4ce4-9600-f06f0a055ce7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 79.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients."
      ],
      "metadata": {
        "id": "O3U542bfTluZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ipython-input-24-9be8c55b9329\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Titanic dataset (you can replace the URL with your local dataset path)\n",
        "# Note this dataframe is from a different URL than the previous files and does not contain the column 'Embarked'\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "# Embarked column does not exist in this dataset\n",
        "data.dropna(subset=['Fare'], inplace=True)\n",
        "\n",
        "# Encode categorical features (e.g., 'Sex', 'Pclass')\n",
        "data = pd.get_dummies(data, columns=['Sex','Pclass'], drop_first=True)\n",
        "\n",
        "# Drop columns that won't be used for the model.\n",
        "# These columns are not present in this dataset.\n",
        "data.drop(columns=['Name'], inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(columns='Survived')  # All columns except 'Survived' are features\n",
        "y = data['Survived']  # The target variable is 'Survived'\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling (Standardization) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get predicted probabilities (needed for ROC-AUC score)\n",
        "y_pred_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "print(f\"ROC-AUC score: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lqc5grB9TrAs",
        "outputId": "2d57e743-a7fd-41c2-9d73-72ccf66ad98e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC score: 0.8169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "Score."
      ],
      "metadata": {
        "id": "t1oYNa0nT-wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ipython-input-26-dfac7d9ef261\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Titanic dataset (you can replace the URL with your local dataset path)\n",
        "# Updated to use the same dataset with Embarked column as in ipython-input-20-e7e00c7a3ce7\n",
        "url_titanic = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "data = pd.read_csv(url_titanic)\n",
        "\n",
        "# Handle missing values\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "data.dropna(subset=['Fare'], inplace=True)\n",
        "\n",
        "# Drop columns that won't be used for the model\n",
        "data.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'], inplace=True)\n",
        "\n",
        "# Encode categorical features (e.g., 'Sex', 'Embarked', 'Pclass')\n",
        "data = pd.get_dummies(data, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\n",
        "\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(columns='Survived')  # All columns except 'Survived' are features\n",
        "y = data['Survived']  # The target variable is 'Survived'\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling (Standardization) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train a Logistic Regression model with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with C=0.5: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2imPWXBUEe8",
        "outputId": "2e6668f2-4d12-4198-9b62-ece892911af9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 79.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classificatio."
      ],
      "metadata": {
        "id": "mukdlv55UcRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ipython-input-28-c625a5247c15\n",
        "# Then we can import the modules.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Titanic dataset (using the correct URL with Embarked column)\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True) # This column is now in the dataframe\n",
        "data.dropna(subset=['Fare'], inplace=True)\n",
        "\n",
        "# Encode categorical features (e.g., 'Sex', 'Pclass', 'Embarked')\n",
        "data = pd.get_dummies(data, columns=['Sex','Pclass', 'Embarked'], drop_first=True)\n",
        "\n",
        "# Drop columns that won't be used for the model.\n",
        "# We need to drop the same columns in each notebook.\n",
        "data.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'], inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(columns='Survived')  # All columns except 'Survived' are features\n",
        "y = data['Survived']  # The target variable is 'Survived'\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling (Standardization) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get predicted probabilities (needed for ROC-AUC score)\n",
        "y_pred_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "print(f\"ROC-AUC score: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbTrfuP4Ukxx",
        "outputId": "814c7a12-8db1-4f81-d25f-3acc41d2c8ed"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC score: 0.8763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy."
      ],
      "metadata": {
        "id": "vxGo7mQcVUhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ipython-input-30-0a9332d27924\n",
        "# we need to explicitly import modules\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Titanic dataset (using the correct URL with Embarked column)\n",
        "# We are now using the same URL as the one in ipython-input-32-0a9332d27924\n",
        "url_titanic = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "data = pd.read_csv(url_titanic)\n",
        "\n",
        "# Handle missing values\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "data.dropna(subset=['Fare'], inplace=True)\n",
        "\n",
        "# Drop columns that won't be used for the model\n",
        "data.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'], inplace=True)\n",
        "\n",
        "# Encode categorical features (e.g., 'Sex', 'Embarked', 'Pclass')\n",
        "data = pd.get_dummies(data, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\n",
        "\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(columns='Survived')  # All columns except 'Survived' are features\n",
        "y = data['Survived']  # The target variable is 'Survived'\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling (Standardization) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train a Logistic Regression model with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with C=0.5: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7bVhk2nVcRu",
        "outputId": "e68ec3e7-2d1e-46b2-9e66-5ba2ec76d9dd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 79.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)."
      ],
      "metadata": {
        "id": "V-5C3SpMV9fF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ipython-input-36-11483450ef3e\n",
        "\n",
        "# we need to explicitly import modules\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Titanic dataset (using the correct URL with Embarked column)\n",
        "# this url is now consistent with the other notebooks\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "data.dropna(subset=['Fare'], inplace=True)\n",
        "\n",
        "# Encode categorical features (e.g., 'Sex', 'Embarked', 'Pclass')\n",
        "data = pd.get_dummies(data, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\n",
        "\n",
        "# Drop columns that won't be used for the model\n",
        "data.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'], inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(columns='Survived')  # All columns except 'Survived' are features\n",
        "y = data['Survived']  # The target variable is 'Survived'\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling (Standardization) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Compute the Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Print the MCC score\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89vjxrAgWCcw",
        "outputId": "a2b2af04-9321-41f2-dd63-8ad939d2bef6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.5817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "uAAgvHRrWM-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to explicitly import modules\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Titanic dataset (using the correct URL with Embarked column)\n",
        "# Changed to the correct URL that has the Embarked column.\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "data.dropna(subset=['Fare'], inplace=True)\n",
        "\n",
        "# Encode categorical features (e.g., 'Sex', 'Embarked', 'Pclass')\n",
        "data = pd.get_dummies(data, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\n",
        "\n",
        "# Drop columns that won't be used for the model\n",
        "data.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'], inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(columns='Survived')  # All columns except 'Survived' are features\n",
        "y = data['Survived']  # The target variable is 'Survived'\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression on Raw Data (No Scaling)\n",
        "model_raw = LogisticRegression(max_iter=1000)\n",
        "model_raw.fit(X_train, y_train)  # Train the model on raw (unscaled) data\n",
        "y_pred_raw = model_raw.predict(X_test)  # Predict on the test set\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)  # Calculate accuracy for raw data\n",
        "\n",
        "# Apply feature scaling (Standardization) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on Standardized Data\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)  # Train the model on standardized data\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)  # Predict on the scaled test set\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)  # Calculate accuracy for scaled data\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"Accuracy on Raw Data: {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy on Standardized Data: {accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFXSqgFyWe_l",
        "outputId": "e1c8822b-103f-44fb-e72e-d6fb1e4ff02f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Raw Data: 0.7989\n",
            "Accuracy on Standardized Data: 0.7989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation."
      ],
      "metadata": {
        "id": "UKxrnNSAW1Nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to explicitly import modules\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset (using the correct URL with Embarked column)\n",
        "# Changed to the correct URL that has the Embarked column.\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv' #This is the correct url\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "data.dropna(subset=['Fare'], inplace=True)\n",
        "\n",
        "# Encode categorical features (e.g., 'Sex', 'Embarked', 'Pclass')\n",
        "data = pd.get_dummies(data, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\n",
        "\n",
        "# Drop columns that won't be used for the model\n",
        "data.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'], inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(columns='Survived')  # All columns except 'Survived' are features\n",
        "y = data['Survived']  # The target variable is 'Survived'\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling (Standardization) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define the parameter grid for C\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} #Corrected this param_grid\n",
        "\n",
        "# Perform GridSearchCV to find the optimal value of C\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print the best hyperparameter (C) and the corresponding accuracy\n",
        "best_C = grid_search.best_params_['C']\n",
        "print(f\"Best C: {best_C}\")\n",
        "\n",
        "# Evaluate the model on the test set using the best C\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of the model with the optimal C: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3SPUlfOW45C",
        "outputId": "d65b159b-fa1c-4757-8e37-46ba398e9a5a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C: 0.1\n",
            "Accuracy of the model with the optimal C: 0.7989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions."
      ],
      "metadata": {
        "id": "NQtkNCieW9-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install joblib\n",
        "!pip install joblib\n",
        "\n",
        "# we need to explicitly import modules\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib  # To save and load the model\n",
        "\n",
        "# Load Titanic dataset (replace with your local dataset path if needed)\n",
        "# Changed to the correct URL that has the Embarked column.\n",
        "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv' #This is the correct url\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True) # This line needs to remain since this dataset has Embarked\n",
        "data.dropna(subset=['Fare'], inplace=True)\n",
        "\n",
        "# Encode categorical features (e.g., 'Sex', 'Embarked', 'Pclass')\n",
        "data = pd.get_dummies(data, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)\n",
        "\n",
        "# Drop columns that won't be used for the model\n",
        "data.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'], inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(columns='Survived')  # All columns except 'Survived' are features\n",
        "y = data['Survived']  # The target variable is 'Survived'\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling (Standardization) to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Save the trained model and scaler using joblib\n",
        "joblib.dump(model, 'logistic_regression_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')  # Save the scaler as well\n",
        "\n",
        "# Load the trained model and scaler\n",
        "loaded_model = joblib.load('logistic_regression_model.pkl')\n",
        "loaded_scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "# Use the loaded model to make predictions on the test set\n",
        "y_pred = loaded_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model's accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the model after loading: {accuracy:.4f}\")\n",
        "\n",
        "# Example: Predict on a new sample (this data should be scaled using the loaded scaler)\n",
        "new_data = [[30.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]]  # Example new data point (needs to match the features)\n",
        "new_data_scaled = loaded_scaler.transform(new_data)  # Apply scaling to new data\n",
        "new_prediction = loaded_model.predict(new_data_scaled)\n",
        "\n",
        "print(f\"Prediction for new data: {new_prediction[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ROuhpiSZRwb",
        "outputId": "b5ef25d1-29d8-4f79-df29-e4af5c0a8c18"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Accuracy of the model after loading: 0.7989\n",
            "Prediction for new data: 0\n"
          ]
        }
      ]
    }
  ]
}